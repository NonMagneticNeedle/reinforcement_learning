{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "muzero_simple.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPE5arNvjBc0v3UQrjQd2F2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/muzero/muzero_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poo2Aq6q1-HU"
      },
      "source": [
        "**simple muzero without self play,priority sampling and reanalyze(target value network)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTIHZ0iX_g2w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQRen3PlNkiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e53869-8ea4-4235-ac56-20468e3fc500"
      },
      "source": [
        "import numpy as np\r\n",
        "def stcat(x,support=5):\r\n",
        "  x = np.sign(x) * ((abs(x) + 1)**0.5 - 1) + 0.001 * x\r\n",
        "  x = np.clip(x, -support, support)\r\n",
        "  floor = np.floor(x)\r\n",
        "  prob = x - floor\r\n",
        "  logits = np.zeros( 2 * support + 1)\r\n",
        "  first_index = int(floor + support)\r\n",
        "  second_index = int(floor + support+1)\r\n",
        "  logits[first_index] = 1-prob\r\n",
        "  if prob>0:\r\n",
        "    logits[second_index] = prob\r\n",
        "  return logits\r\n",
        "def catts(x,support=5):\r\n",
        "  support = np.arange(-support, support+1, 1)\r\n",
        "  x = np.sum(support*x)\r\n",
        "  x = np.sign(x) * ((((1 + 4 * 0.001 * (abs(x) + 1 + 0.001))**0.5 - 1) / (2 * 0.001))** 2- 1)\r\n",
        "  return x  \r\n",
        "\r\n",
        "#cat = stcat(58705)\r\n",
        "#print(cat)\r\n",
        "#scalar = catts(cat)\r\n",
        "#print(scalar)\r\n",
        "print(\"done\")        \r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhf3dyDa2GYq",
        "outputId": "06291793-d055-41e1-bf6c-436f7c798391"
      },
      "source": [
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class MuZeroNet(nn.Module):\r\n",
        "    def __init__(self, input_size, action_space_n, reward_support_size, value_support_size):\r\n",
        "        super().__init__()\r\n",
        "        self.hx_size = 32\r\n",
        "        self._representation = nn.Sequential(nn.Linear(input_size, self.hx_size),\r\n",
        "                                             nn.Tanh())\r\n",
        "        self._dynamics_state = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\r\n",
        "                                             nn.Tanh(),\r\n",
        "                                             nn.Linear(64, self.hx_size),\r\n",
        "                                             nn.Tanh())\r\n",
        "        self._dynamics_reward = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\r\n",
        "                                              nn.LeakyReLU(),\r\n",
        "                                              nn.Linear(64, 2*reward_support_size+1))\r\n",
        "        self._prediction_actor = nn.Sequential(nn.Linear(self.hx_size, 64),\r\n",
        "                                               nn.LeakyReLU(),\r\n",
        "                                               nn.Linear(64, action_space_n))\r\n",
        "        self._prediction_value = nn.Sequential(nn.Linear(self.hx_size, 64),\r\n",
        "                                               nn.LeakyReLU(),\r\n",
        "                                               nn.Linear(64, 2*value_support_size+1))\r\n",
        "        self.action_space_n = action_space_n\r\n",
        "\r\n",
        "        self._prediction_value[-1].weight.data.fill_(0)\r\n",
        "        self._prediction_value[-1].bias.data.fill_(0)\r\n",
        "        self._dynamics_reward[-1].weight.data.fill_(0)\r\n",
        "        self._dynamics_reward[-1].bias.data.fill_(0)\r\n",
        "\r\n",
        "    def p(self, state):\r\n",
        "        actor_logit = torch.softmax(self._prediction_actor(state),dim=1)\r\n",
        "        value = torch.softmax(self._prediction_value(state),dim=1)\r\n",
        "        return actor_logit, value\r\n",
        "\r\n",
        "    def h(self, obs_history):\r\n",
        "        return self._representation(obs_history)\r\n",
        "\r\n",
        "    def g(self, state, action):\r\n",
        "        x = torch.cat((state, action), dim=1)\r\n",
        "        next_state = self._dynamics_state(x)\r\n",
        "        reward = torch.softmax(self._dynamics_reward(x),dim=1)\r\n",
        "        return next_state, reward     \r\n",
        "\r\n",
        "    def initial_state(self, x):\r\n",
        "        hout = self.h(x)\r\n",
        "        prob,v= self.p(hout)\r\n",
        "        return hout,prob,v\r\n",
        "    def next_state(self,hin,a):\r\n",
        "        hout,r = self.g(hin,a)\r\n",
        "        prob,v= self.p(hout)\r\n",
        "        return hout,r,prob,v\r\n",
        "    def inference_initial_state(self, x):\r\n",
        "        with torch.no_grad():\r\n",
        "          hout = self.h(x)\r\n",
        "          prob,v=self.p(hout)\r\n",
        "\r\n",
        "          return hout,prob,v\r\n",
        "    def inference_next_state(self,hin,a):\r\n",
        "        with torch.no_grad():\r\n",
        "          hout,r = self.g(hin,a)\r\n",
        "          prob,v=self.p(hout)\r\n",
        "          return hout,r,prob,v     \r\n",
        "\r\n",
        "\r\n",
        "#net = Net(observation_size=4,hidden_dim=4,action_dim=2)\r\n",
        "#net = MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5)\r\n",
        "print(\"done\")                                      "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yAoDfNkXvJk",
        "outputId": "b6f3e49d-4703-486d-f735-021f5a5c900c"
      },
      "source": [
        "\r\n",
        "#MTCS    MUzero modified for intermeditate rewards settings and using predicted rewards\r\n",
        "#accepts policy as a list\r\n",
        "import torch\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "def dynamics(net,state,action):\r\n",
        "    #print(state,action) \r\n",
        "    next_state,reward,prob,value = net.inference_next_state(state,torch.tensor([action]).float())\r\n",
        "    reward = catts(reward.numpy().ravel())\r\n",
        "    value = catts(value.numpy().ravel())\r\n",
        "    prob = prob.tolist()[0]\r\n",
        "    #print(\"dynamics\",prob)\r\n",
        "    return next_state,reward,prob,value\r\n",
        "\r\n",
        "\r\n",
        "def ucb_score(parent, child,min_max):\r\n",
        "    \"\"\"\r\n",
        "    The score for an action that would transition between the parent and child.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    c1 = 1.25\r\n",
        "    c2 = 19652\r\n",
        "    visits = np.array([child.visit_count for child in parent.children.values()])\r\n",
        "    #print(\"child visits in ucb score\",visits,\"sum of all child visits\",np.sum(visits),\"parent visits\",parent.visit_count)\r\n",
        "\r\n",
        "    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)*(c1 + math.log((parent.visit_count+c2+1)/c2))\r\n",
        "\r\n",
        "    # The value of the child is from the perspective of the opposing player\r\n",
        "    value_score = min_max.normalize(child.value()) ####################################################################################################\r\n",
        "    return value_score + prior_score\r\n",
        "\r\n",
        "def softmax(x):\r\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\r\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\r\n",
        "class Node:\r\n",
        "    def __init__(self, prior):\r\n",
        "        self.visit_count = 0\r\n",
        "        self.prior = prior\r\n",
        "        self.q = 0\r\n",
        "        self.reward = 0\r\n",
        "        self.children = {}\r\n",
        "        self.state = None\r\n",
        "\r\n",
        "    def expanded(self):\r\n",
        "        return len(self.children) > 0\r\n",
        "\r\n",
        "    def value(self):\r\n",
        "        if self.visit_count == 0:\r\n",
        "            return 0\r\n",
        "        return self.q/self.visit_count\r\n",
        "\r\n",
        "    def select_action(self, temperature):\r\n",
        "        \"\"\"\r\n",
        "        Select action according to the visit count distribution fo child nodes and the temperature.\r\n",
        "        \"\"\"\r\n",
        "        visit_counts = np.array([child.visit_count for child in self.children.values()])\r\n",
        "        actions = [action for action in self.children.keys()]\r\n",
        "        if temperature == 0:\r\n",
        "            action = actions[np.argmax(visit_counts)]\r\n",
        "            #prob= softmax(visit_counts / sum(visit_counts))\r\n",
        "            prob= visit_counts / sum(visit_counts)\r\n",
        "        elif temperature == float(\"inf\"):\r\n",
        "            action = np.random.choice(actions)\r\n",
        "        else:\r\n",
        "            # See paper appendix Data Generation\r\n",
        "            visit_count_distribution = visit_counts ** (1 / temperature)\r\n",
        "            #visit_count_distribution = softmax(visit_count_distribution/ sum(visit_count_distribution))\r\n",
        "            counts_exp = np.exp(visit_counts) * (1 / temperature)\r\n",
        "            softmax_probs = counts_exp / np.sum(counts_exp, axis=0) \r\n",
        "            prob = visit_count_distribution/ sum(visit_count_distribution)\r\n",
        "            \r\n",
        "            action = np.random.choice(actions, p=softmax_probs)\r\n",
        "        #prob =needs softmax????    \r\n",
        "        mcts_value = self.value()\r\n",
        "        return action,prob,mcts_value\r\n",
        "\r\n",
        "    def select_child(self,min_max):\r\n",
        "        \"\"\"\r\n",
        "        Select the child with the highest UCB score.\r\n",
        "        \"\"\"\r\n",
        "        #if paret visit = 0 select random child?\r\n",
        "        best_score = -np.inf\r\n",
        "        best_action = -1\r\n",
        "        best_child = None\r\n",
        "        if self.visit_count == 0:\r\n",
        "            return random.sample(self.children.items(), 1)[0]\r\n",
        "        for action, child in self.children.items():\r\n",
        "            score = ucb_score(self, child,min_max)#############################################\r\n",
        "            if score > best_score:\r\n",
        "                best_score = score\r\n",
        "                best_action = action\r\n",
        "                best_child = child\r\n",
        "\r\n",
        "        return best_action, best_child\r\n",
        "\r\n",
        "    def expand(self, state, action_probs, reward):\r\n",
        "        \"\"\"\r\n",
        "        We expand a node and keep track of the prior policy probability given by neural network\r\n",
        "        \"\"\"\r\n",
        "        self.state  = state\r\n",
        "        self.reward = reward\r\n",
        "        for a, prob in enumerate(action_probs):\r\n",
        "            \r\n",
        "            self.children[a] = Node(prior=prob)\r\n",
        "    def __repr__(self):\r\n",
        "        \"\"\"\r\n",
        "        Debugger pretty print node info\r\n",
        "        \"\"\"\r\n",
        "        prior = \"{0:.2f}\".format(self.prior)\r\n",
        "        return \"{} Prior: {} Count: {} Value: {}\".format(self.state.__str__(), prior, self.visit_count, self.value())\r\n",
        "\r\n",
        "class MinMaxStats:\r\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.MAXIMUM_FLOAT_VALUE = float('inf')\r\n",
        "        self.maximum =  -1*self.MAXIMUM_FLOAT_VALUE\r\n",
        "        self.minimum =  self.MAXIMUM_FLOAT_VALUE\r\n",
        "\r\n",
        "    def update(self, value: float):\r\n",
        "        if value is None:\r\n",
        "            raise ValueError\r\n",
        "\r\n",
        "        self.maximum = max(self.maximum, value)\r\n",
        "        self.minimum = min(self.minimum, value)\r\n",
        "\r\n",
        "    def normalize(self, value: float) -> float:\r\n",
        "        # If the value is unknow, by default we set it to the minimum possible value\r\n",
        "        if value is None:\r\n",
        "            return 0.0\r\n",
        "\r\n",
        "        if self.maximum > self.minimum:\r\n",
        "            # We normalize only when we have set the maximum and minimum values.\r\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\r\n",
        "        return value\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def MTCSbackup(search_path, value,gamma,min_max):\r\n",
        "    \"\"\"\r\n",
        "    At the end of a simulation, we propagate the evaluation all the way up the tree\r\n",
        "    to the root.\r\n",
        "    \"\"\"\r\n",
        "    for node in reversed(search_path):\r\n",
        "        node.q+=value\r\n",
        "        node.visit_count += 1\r\n",
        "        min_max.update(node.value())\r\n",
        "        \r\n",
        "        value = node.reward + gamma*value\r\n",
        "\r\n",
        "def add_exploration_noise_0(priors):\r\n",
        "    root_dirichlet_alpha=0.25\r\n",
        "    noise = np.random.dirichlet([root_dirichlet_alpha] * len(priors)) #dirichlet_alpha=0.03\r\n",
        "    frac = 0.25\r\n",
        "    for i in range(len(priors)):\r\n",
        "        priors[i] = priors[i] *(1 - frac) + noise[i]* frac\r\n",
        "    return priors  \r\n",
        "\r\n",
        "def run_MTCS_ZERO( net, state,prob,value,num_simulations,gamma = 0.9):\r\n",
        "\r\n",
        "    \r\n",
        "    root = Node(0)\r\n",
        "    min_max= MinMaxStats()\r\n",
        "    # EXPAND root\r\n",
        "    # ADD DiChlet Noise to the priors for exploration \r\n",
        "    action_probs, value = prob.tolist()[0] ,catts(value.numpy().ravel())#prediction using model #make it into list and scalar \r\n",
        "    \r\n",
        "    action_probs = add_exploration_noise_0(action_probs)\r\n",
        "    #print(\"mtcs start prob,value: \",action_probs, value )\r\n",
        "    \r\n",
        "\r\n",
        "    num_actions = len(action_probs)\r\n",
        "    #root.mean_q = value  #####do we need it?No sice expand doesnt increase the visit cout so it will become mean q = value/0 that is infiity \r\n",
        "    \r\n",
        "    root.expand(state, action_probs,0)\r\n",
        "\r\n",
        "\r\n",
        "    for _ in range(num_simulations):\r\n",
        "        node = root\r\n",
        "        search_path = [node]\r\n",
        "\r\n",
        "        # SELECT\r\n",
        "        while node.expanded():\r\n",
        "            action, node = node.select_child(min_max) ##################################################################need to pass mimax here for the ucb calculation\r\n",
        "            search_path.append(node)\r\n",
        "\r\n",
        "        parent = search_path[-2]\r\n",
        "        state = parent.state\r\n",
        "        # Now we're at a leaf node and we would like to expand\r\n",
        "        action = onehot(action,num_actions) #the action needs to be in one hot for dynaics\r\n",
        "        next_state,r,action_probs, value = dynamics(net,state,action) \r\n",
        "        # EXPAND\r\n",
        "        node.expand(next_state, action_probs,r)\r\n",
        "\r\n",
        "        MTCSbackup(search_path, value,gamma,min_max)#######################################\r\n",
        "\r\n",
        "    return root\r\n",
        "print(\"done\")        \r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-NpN4lU12kW"
      },
      "source": [
        "import gym\r\n",
        "class ScalingObservationWrapper(gym.ObservationWrapper):\r\n",
        "    \"\"\"\r\n",
        "    Wrapper that apply a min-max scaling of observations.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, env, low=None, high=None):\r\n",
        "        super().__init__(env)\r\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\r\n",
        "\r\n",
        "        low = np.array(self.observation_space.low if low is None else low)\r\n",
        "        high = np.array(self.observation_space.high if high is None else high)\r\n",
        "\r\n",
        "        self.mean = (high + low) / 2\r\n",
        "        self.max = high - self.mean\r\n",
        "\r\n",
        "    def observation(self, observation):\r\n",
        "        return (observation - self.mean) / self.max"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVGfWytBB1"
      },
      "source": [
        "\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from tqdm import tqdm\r\n",
        "def onehot(a,n=2):\r\n",
        "  return np.eye(n)[a]\r\n",
        "def play_game(env,net,n_sim,discount,render):\r\n",
        "    trajectory=[]\r\n",
        "    state = env.reset() \r\n",
        "    done = False\r\n",
        "    while not done:\r\n",
        "        if render:\r\n",
        "          env.render()\r\n",
        "        h ,prob,value= net.inference_initial_state(torch.tensor([state]).float()) \r\n",
        "        action,action_prob,mcts_val  =run_MTCS_ZERO( net,h,prob,value,num_simulations=n_sim,gamma=discount).select_action(temperature=1)\r\n",
        "        value=mcts_val \r\n",
        "        next_state, reward, done, info = env.step(action)\r\n",
        "        data = (state,onehot(action),action_prob,mcts_val,reward)\r\n",
        "        trajectory.append(data)\r\n",
        "        state = next_state\r\n",
        "    print(\"played for \",len(trajectory),\" steps\")   \r\n",
        "    return trajectory    \r\n",
        "\r\n",
        "def sample_games(buffer,batch_size):\r\n",
        "    # Sample game from buffer either uniformly or according to some priority\r\n",
        "    #print(\"samplig from .\",len(buffer))\r\n",
        "    return random.choices(buffer, k=batch_size)\r\n",
        "\r\n",
        "def sample_position(trajectory):\r\n",
        "    # Sample position from game either uniformly or according to some priority.\r\n",
        "    return np.random.randint(0, len(trajectory))\r\n",
        "\r\n",
        "\r\n",
        "def sample_batch(action_space_size,buffer,discount,batch_size,num_unroll_steps, td_steps):\r\n",
        "    obs_batch, action_batch, reward_batch, value_batch, policy_batch = [], [], [], [], []\r\n",
        "    games = sample_games(buffer,batch_size)\r\n",
        "    for g in games:\r\n",
        "      game_pos = sample_position(g)#state index\r\n",
        "      state,action,action_prob,root_val,reward = zip(*g)\r\n",
        "      state,action,action_prob,root_val,reward =list(state),list(action),list(action_prob),list(root_val),list(reward)\r\n",
        "\r\n",
        "      _actions = action[game_pos:game_pos + num_unroll_steps]\r\n",
        "      # random action selection to complete num_unroll_steps\r\n",
        "      _actions += [onehot(np.random.randint(0, action_space_size))for _ in range(num_unroll_steps - len(_actions))]\r\n",
        "\r\n",
        "      obs_batch.append(state[game_pos])\r\n",
        "      action_batch.append(_actions)\r\n",
        "      value, reward, policy = make_target(child_visits=action_prob ,root_values=root_val,rewards=reward,state_index=game_pos,discount=discount, num_unroll_steps=num_unroll_steps, td_steps=td_steps)\r\n",
        "      reward_batch.append(reward)\r\n",
        "      value_batch.append(value)\r\n",
        "      policy_batch.append(policy)\r\n",
        "\r\n",
        "    obs_batch = torch.tensor(obs_batch).float()\r\n",
        "    action_batch = torch.tensor(action_batch).long()\r\n",
        "    reward_batch = torch.tensor(reward_batch).float()\r\n",
        "    value_batch = torch.tensor(value_batch).float()\r\n",
        "    policy_batch = torch.tensor(policy_batch).float()\r\n",
        "    return obs_batch, action_batch, reward_batch, value_batch, policy_batch\r\n",
        "\r\n",
        "\r\n",
        "def make_target(child_visits,root_values,rewards,state_index,discount=0.99, num_unroll_steps=5, td_steps=10):\r\n",
        "        # The value target is the discounted root value of the search tree N steps into the future, plus\r\n",
        "        # the discounted sum of all rewards until then.\r\n",
        "        target_values, target_rewards, target_policies = [], [], []\r\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\r\n",
        "            bootstrap_index = current_index + td_steps\r\n",
        "            if bootstrap_index < len(root_values):\r\n",
        "                value = root_values[bootstrap_index] * discount ** td_steps\r\n",
        "            else:\r\n",
        "                value = 0\r\n",
        "\r\n",
        "            for i, reward in enumerate(rewards[current_index:bootstrap_index]):\r\n",
        "                value += reward * discount ** i\r\n",
        "\r\n",
        "            if current_index < len(root_values):\r\n",
        "                target_values.append(stcat(value))\r\n",
        "                target_rewards.append(stcat(rewards[current_index]))\r\n",
        "                target_policies.append(child_visits[current_index])\r\n",
        "\r\n",
        "            else:\r\n",
        "                # States past the end of games are treated as absorbing states.\r\n",
        "                target_values.append(stcat(0))\r\n",
        "                target_rewards.append(stcat(0))\r\n",
        "                # Note: Target policy is  set to 0 so that no policy loss is calculated for them\r\n",
        "                #target_policies.append([0 for _ in range(len(child_visits[0]))])\r\n",
        "                target_policies.append(child_visits[0]*0.0)\r\n",
        "\r\n",
        "        return target_values, target_rewards, target_policies\r\n",
        "\r\n",
        "\r\n",
        "def scalar_reward_loss( prediction, target):\r\n",
        "        return -(torch.log(prediction) * target).sum(1)\r\n",
        "\r\n",
        "def scalar_value_loss( prediction, target):\r\n",
        "        return -(torch.log(prediction) * target).sum(1)\r\n",
        "def update_weights(model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps ):\r\n",
        "    batch = sample_batch(action_space_size,replay_buffer,discount,batch_size,num_unroll_steps, td_steps)\r\n",
        "    obs_batch, action_batch, target_reward, target_value, target_policy = batch\r\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "    obs_batch = obs_batch.to(device)\r\n",
        "    action_batch = action_batch.to(device)#.unsqueeze(-1) # its not onehot yet \r\n",
        "    target_reward = target_reward.to(device)\r\n",
        "    target_value = target_value.to(device)\r\n",
        "    target_policy = target_policy.to(device)\r\n",
        "\r\n",
        "    # transform targets to categorical representation # its already done\r\n",
        "    # Reference:  Appendix F\r\n",
        "    #transformed_target_reward = config.scalar_transform(target_reward)\r\n",
        "    target_reward_phi =target_reward #config.reward_phi(transformed_target_reward)\r\n",
        "    #transformed_target_value = config.scalar_transform(target_value)\r\n",
        "    target_value_phi = target_value#config.value_phi(transformed_target_value)\r\n",
        "\r\n",
        "    hidden_state, policy_prob,value  = model.initial_state(obs_batch) # initial model_call ###################################### make changes\r\n",
        "    #h,init_pred_p,init_pred_v = net.initial_state(in_s)\r\n",
        "\r\n",
        "    value_loss = scalar_value_loss(value, target_value_phi[:, 0])\r\n",
        "    policy_loss = -(torch.log(policy_prob) * target_policy[:, 0]).sum(1)\r\n",
        "    reward_loss = torch.zeros(batch_size, device=device)\r\n",
        "\r\n",
        "    gradient_scale = 1 / num_unroll_steps\r\n",
        "    for step_i in range(num_unroll_steps):\r\n",
        "        hidden_state, reward,policy_prob,value  = model.next_state(hidden_state, action_batch[:, step_i]) ######################### make changes\r\n",
        "        #h,pred_reward,pred_policy,pred_value= net.next_state(h,act)\r\n",
        "        policy_loss += -(torch.log(policy_prob) * target_policy[:, step_i + 1]).sum(1)\r\n",
        "        value_loss += scalar_value_loss(value, target_value_phi[:, step_i + 1])\r\n",
        "        reward_loss += scalar_reward_loss(reward, target_reward_phi[:, step_i])\r\n",
        "        hidden_state.register_hook(lambda grad: grad * 0.5)\r\n",
        "\r\n",
        "    # optimize\r\n",
        "    value_loss_coeff = 1\r\n",
        "    loss = (policy_loss + value_loss_coeff * value_loss + reward_loss) # find value loss coefficiet = 1?\r\n",
        "    weights = 1\r\n",
        "    weighted_loss = (weights * loss).mean()#1?\r\n",
        "    weighted_loss.register_hook(lambda grad: grad * gradient_scale)\r\n",
        "    loss = loss.mean()\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    weighted_loss.backward()\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)#5?\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "def adjust_lr(optimizer, step_count):\r\n",
        "\r\n",
        "    lr_init=0.05\r\n",
        "    lr_decay_rate=0.01\r\n",
        "    lr_decay_steps=10000\r\n",
        "    lr = lr_init * lr_decay_rate ** (step_count / lr_decay_steps)\r\n",
        "    lr = max(lr, 0.001)\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        param_group['lr'] = lr\r\n",
        "    return lr\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "def net_train(net,  action_space_size, replay_buffer,discount,batch_size,num_unroll_steps, td_steps):\r\n",
        "    model =MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5) #net\r\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9,weight_decay=1e-4)\r\n",
        "    training_steps=5000#20000\r\n",
        "    # wait for replay buffer to be non-empty\r\n",
        "    while len(replay_buffer) == 0:\r\n",
        "        pass\r\n",
        "\r\n",
        "    for step_count in tqdm(range(training_steps)):\r\n",
        "        lr = adjust_lr( optimizer, step_count)\r\n",
        "        update_weights(model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps)\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZWra51wFVvb",
        "outputId": "e23551ba-673f-42ac-9c6d-131dbcd925e1"
      },
      "source": [
        "import gym\r\n",
        "import numpy as np\r\n",
        "buffer =[]\r\n",
        "\r\n",
        "render = False\r\n",
        "episodes_per_train=20\r\n",
        "training_steps=50\r\n",
        "epochs=50\r\n",
        "n_sim= 50\r\n",
        "discount = 0.99\r\n",
        "batch_size = 128\r\n",
        "envs = ['CartPole-v1']\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "env=gym.make(envs[0])\r\n",
        "env = ScalingObservationWrapper(env, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5])\r\n",
        "#env=env.unwrapped\r\n",
        "\r\n",
        "s_dim =env.observation_space.shape[0]\r\n",
        "print(\"s_dim: \",s_dim)\r\n",
        "a_dim =env.action_space.n\r\n",
        "print(\"a_dim: \",a_dim)\r\n",
        "a_bound =1 #env.action_space.high[0]\r\n",
        "print(\"a_bound: \",a_bound)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#net = Net(observation_size=4,hidden_dim=4,action_dim=2).to_device(device)\r\n",
        "net = MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5)\r\n",
        "\r\n",
        "for t in range(training_steps):\r\n",
        "  for _ in range(episodes_per_train):\r\n",
        "    buffer.append(play_game(env,net,n_sim,discount,render))\r\n",
        "  print(\"training\")  \r\n",
        "  net = net_train(net,  action_space_size=a_dim, replay_buffer=buffer,discount=discount,batch_size=batch_size,num_unroll_steps=5, td_steps=10)\r\n",
        "  \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s_dim:  4\n",
            "a_dim:  2\n",
            "a_bound:  1\n",
            "played for  11  steps\n",
            "played for  17  steps\n",
            "played for  11  steps\n",
            "played for  21  steps\n",
            "played for  17  steps\n",
            "played for  11  steps\n",
            "played for  16  steps\n",
            "played for  29  steps\n",
            "played for  21  steps\n",
            "played for  10  steps\n",
            "played for  18  steps\n",
            "played for  25  steps\n",
            "played for  12  steps\n",
            "played for  17  steps\n",
            "played for  18  steps\n",
            "played for  10  steps\n",
            "played for  22  steps\n",
            "played for  14  steps\n",
            "played for  13  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/5000 [00:00<04:46, 17.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "played for  23  steps\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [04:46<00:00, 17.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "played for  124  steps\n",
            "played for  147  steps\n",
            "played for  130  steps\n",
            "played for  137  steps\n",
            "played for  140  steps\n",
            "played for  168  steps\n",
            "played for  127  steps\n",
            "played for  133  steps\n",
            "played for  131  steps\n",
            "played for  138  steps\n",
            "played for  110  steps\n",
            "played for  132  steps\n",
            "played for  128  steps\n",
            "played for  112  steps\n",
            "played for  134  steps\n",
            "played for  138  steps\n",
            "played for  131  steps\n",
            "played for  120  steps\n",
            "played for  118  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/5000 [00:00<04:46, 17.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "played for  117  steps\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 4530/5000 [04:28<00:27, 17.37it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3XjzidmtzwX"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}