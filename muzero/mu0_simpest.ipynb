{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mu0_simpest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYBO+5YUbUElxdPLcGBtxP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/muzero/mu0_simpest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2remTyGXKQ2M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQRen3PlNkiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca4f760-8bf1-4965-d098-48aed156e510"
      },
      "source": [
        "import numpy as np\r\n",
        "def stcat(x,support=5):\r\n",
        "  x = np.sign(x) * ((abs(x) + 1)**0.5 - 1) + 0.001 * x\r\n",
        "  x = np.clip(x, -support, support)\r\n",
        "  floor = np.floor(x)\r\n",
        "  prob = x - floor\r\n",
        "  logits = np.zeros( 2 * support + 1)\r\n",
        "  first_index = int(floor + support)\r\n",
        "  second_index = int(floor + support+1)\r\n",
        "  logits[first_index] = 1-prob\r\n",
        "  if prob>0:\r\n",
        "    logits[second_index] = prob\r\n",
        "  return logits\r\n",
        "def catts(x,support=5):\r\n",
        "  support = np.arange(-support, support+1, 1)\r\n",
        "  x = np.sum(support*x)\r\n",
        "  x = np.sign(x) * ((((1 + 4 * 0.001 * (abs(x) + 1 + 0.001))**0.5 - 1) / (2 * 0.001))** 2- 1)\r\n",
        "  return x  \r\n",
        "\r\n",
        "#cat = stcat(58705)\r\n",
        "#print(cat)\r\n",
        "#scalar = catts(cat)\r\n",
        "#print(scalar)\r\n",
        "print(\"done\")        \r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhf3dyDa2GYq",
        "outputId": "cd4816c0-cd91-443d-a32e-91531a059265"
      },
      "source": [
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class MuZeroNet(nn.Module):\r\n",
        "    def __init__(self, input_size, action_space_n, reward_support_size, value_support_size):\r\n",
        "        super().__init__()\r\n",
        "        self.hx_size = 32\r\n",
        "        self._representation = nn.Sequential(nn.Linear(input_size, self.hx_size),\r\n",
        "                                             nn.Tanh())\r\n",
        "        self._dynamics_state = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\r\n",
        "                                             nn.Tanh(),\r\n",
        "                                             nn.Linear(64, self.hx_size),\r\n",
        "                                             nn.Tanh())\r\n",
        "        self._dynamics_reward = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\r\n",
        "                                              nn.LeakyReLU(),\r\n",
        "                                              nn.Linear(64, 2*reward_support_size+1))\r\n",
        "        self._prediction_actor = nn.Sequential(nn.Linear(self.hx_size, 64),\r\n",
        "                                               nn.LeakyReLU(),\r\n",
        "                                               nn.Linear(64, action_space_n))\r\n",
        "        self._prediction_value = nn.Sequential(nn.Linear(self.hx_size, 64),\r\n",
        "                                               nn.LeakyReLU(),\r\n",
        "                                               nn.Linear(64, 2*value_support_size+1))\r\n",
        "        self.action_space_n = action_space_n\r\n",
        "\r\n",
        "        self._prediction_value[-1].weight.data.fill_(0)\r\n",
        "        self._prediction_value[-1].bias.data.fill_(0)\r\n",
        "        self._dynamics_reward[-1].weight.data.fill_(0)\r\n",
        "        self._dynamics_reward[-1].bias.data.fill_(0)\r\n",
        "\r\n",
        "    def p(self, state):\r\n",
        "        actor_logit = torch.softmax(self._prediction_actor(state),dim=1)\r\n",
        "        value = torch.softmax(self._prediction_value(state),dim=1)\r\n",
        "        return actor_logit, value\r\n",
        "\r\n",
        "    def h(self, obs_history):\r\n",
        "        return self._representation(obs_history)\r\n",
        "\r\n",
        "    def g(self, state, action):\r\n",
        "        x = torch.cat((state, action), dim=1)\r\n",
        "        next_state = self._dynamics_state(x)\r\n",
        "        reward = torch.softmax(self._dynamics_reward(x),dim=1)\r\n",
        "        return next_state, reward     \r\n",
        "\r\n",
        "    def initial_state(self, x):\r\n",
        "        hout = self.h(x)\r\n",
        "        prob,v= self.p(hout)\r\n",
        "        return hout,prob,v\r\n",
        "    def next_state(self,hin,a):\r\n",
        "        hout,r = self.g(hin,a)\r\n",
        "        prob,v= self.p(hout)\r\n",
        "        return hout,r,prob,v\r\n",
        "    def inference_initial_state(self, x):\r\n",
        "        with torch.no_grad():\r\n",
        "          hout = self.h(x)\r\n",
        "          prob,v=self.p(hout)\r\n",
        "\r\n",
        "          return hout,prob,v\r\n",
        "    def inference_next_state(self,hin,a):\r\n",
        "        with torch.no_grad():\r\n",
        "          hout,r = self.g(hin,a)\r\n",
        "          prob,v=self.p(hout)\r\n",
        "          return hout,r,prob,v     \r\n",
        "\r\n",
        "\r\n",
        "print(\"done\")                                      "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKMTKa6wYtR"
      },
      "source": [
        "\r\n",
        "#MTCS    MUzero modified for intermeditate rewards settings and using predicted rewards\r\n",
        "#accepts policy as a list\r\n",
        "import torch\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import random\r\n",
        "def dynamics(net,state,action):\r\n",
        "    #print(state,action) \r\n",
        "    next_state,reward,prob,value = net.inference_next_state(state,torch.tensor([action]).float())\r\n",
        "    reward = catts(reward.numpy().ravel())\r\n",
        "    value = catts(value.numpy().ravel())\r\n",
        "    prob = prob.tolist()[0]\r\n",
        "    #print(\"dynamics\",prob)\r\n",
        "    return next_state,reward,prob,value\r\n",
        "\r\n",
        "\r\n",
        "class MinMaxStats:\r\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.MAXIMUM_FLOAT_VALUE = float('inf')       \r\n",
        "        self.maximum =  -self.MAXIMUM_FLOAT_VALUE\r\n",
        "        self.minimum =  self.MAXIMUM_FLOAT_VALUE\r\n",
        "\r\n",
        "    def update(self, value: float):\r\n",
        "        if value is None:\r\n",
        "            raise ValueError\r\n",
        "\r\n",
        "        self.maximum = max(self.maximum, value)\r\n",
        "        self.minimum = min(self.minimum, value)\r\n",
        "\r\n",
        "    def normalize(self, value: float) -> float:\r\n",
        "        # If the value is unknow, by default we set it to the minimum possible value\r\n",
        "        if value is None:\r\n",
        "            return 0.0\r\n",
        "\r\n",
        "        if self.maximum > self.minimum:\r\n",
        "            # We normalize only when we have set the maximum and minimum values.\r\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\r\n",
        "        return value\r\n",
        "\r\n",
        "\r\n",
        "class Node:\r\n",
        "    \"\"\"A class that represent nodes inside the MCTS tree\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, prior: float):\r\n",
        "        self.visit_count = 0\r\n",
        "        self.to_play = -1\r\n",
        "        self.prior = prior\r\n",
        "        self.value_sum = 0\r\n",
        "        self.children = {}\r\n",
        "        self.hidden_state = None\r\n",
        "        self.reward = 0\r\n",
        "\r\n",
        "    def expanded(self):\r\n",
        "        return len(self.children) > 0\r\n",
        "\r\n",
        "    def value(self):\r\n",
        "        if self.visit_count == 0:\r\n",
        "            return None\r\n",
        "        return self.value_sum / self.visit_count\r\n",
        "\r\n",
        "\r\n",
        "def softmax_sample(visit_counts, actions, t):\r\n",
        "    counts_exp = np.exp(visit_counts) * (1 / t)\r\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)\r\n",
        "    action_idx = np.random.choice(len(actions), p=probs)\r\n",
        "    return actions[action_idx]\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"MCTS module: where MuZero thinks inside the tree.\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "def add_exploration_noise( node):\r\n",
        "    \"\"\"\r\n",
        "    At the start of each search, we add dirichlet noise to the prior of the root\r\n",
        "    to encourage the search to explore new actions.\r\n",
        "    \"\"\"\r\n",
        "    actions = list(node.children.keys())\r\n",
        "    noise = np.random.dirichlet([0.25] * len(actions)) # config.root_dirichlet_alpha\r\n",
        "    frac = 0.25#config.root_exploration_fraction\r\n",
        "    for a, n in zip(actions, noise):\r\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def ucb_score(parent, child,min_max_stats):\r\n",
        "    \"\"\"\r\n",
        "    The score for a node is based on its value, plus an exploration bonus based on\r\n",
        "    the prior.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    pb_c_base = 19652\r\n",
        "    pb_c_init = 1.25\r\n",
        "    pb_c = math.log((parent.visit_count + pb_c_base + 1) / pb_c_base) + pb_c_init\r\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\r\n",
        "\r\n",
        "    prior_score = pb_c * child.prior\r\n",
        "    value_score = min_max_stats.normalize(child.value())\r\n",
        "    return  value_score + prior_score \r\n",
        "\r\n",
        "def select_child(node, min_max_stats):\r\n",
        "    \"\"\"\r\n",
        "    Select the child with the highest UCB score.\r\n",
        "    \"\"\"\r\n",
        "    # When the parent visit count is zero, all ucb scores are zeros, therefore we return a random child\r\n",
        "    if node.visit_count == 0:\r\n",
        "        return random.sample(node.children.items(), 1)[0]\r\n",
        "\r\n",
        "    _, action, child = max(\r\n",
        "        (ucb_score(node, child, min_max_stats), action,\r\n",
        "         child) for action, child in node.children.items())\r\n",
        "    return action, child\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def expand_node(node, to_play, actions_space,hidden_state,reward,policy):\r\n",
        "    \"\"\"\r\n",
        "    We expand a node using the value, reward and policy prediction obtained from\r\n",
        "    the neural networks.\r\n",
        "    \"\"\"\r\n",
        "    node.to_play = to_play\r\n",
        "    node.hidden_state = hidden_state\r\n",
        "    node.reward = reward\r\n",
        "    policy = {a:policy[a] for a in actions_space}\r\n",
        "    policy_sum = sum(policy.values())\r\n",
        "    for action, p in policy.items():\r\n",
        "        node.children[action] = Node(p / policy_sum) # not needed since mine are already softmax but its fine \r\n",
        "\r\n",
        "\r\n",
        "def backpropagate(search_path, value,to_play,discount, min_max_stats):\r\n",
        "    \"\"\"\r\n",
        "    At the end of a simulation, we propagate the evaluation all the way up the\r\n",
        "    tree to the root.\r\n",
        "    \"\"\"\r\n",
        "    for node in search_path[::-1]: #[::-1] means reversed\r\n",
        "        node.value_sum += value \r\n",
        "        node.visit_count += 1\r\n",
        "        min_max_stats.update(node.value())\r\n",
        "\r\n",
        "        value = node.reward + discount * value\r\n",
        "\r\n",
        "\r\n",
        "def select_action(node, mode ='softmax'):\r\n",
        "    \"\"\"\r\n",
        "    After running simulations inside in MCTS, we select an action based on the root's children visit counts.\r\n",
        "    During training we use a softmax sample for exploration.\r\n",
        "    During evaluation we select the most visited child.\r\n",
        "    \"\"\"\r\n",
        "    visit_counts = [child.visit_count for child in node.children.values()]\r\n",
        "    actions = [action for action in node.children.keys()]\r\n",
        "    action = None\r\n",
        "    if mode == 'softmax':\r\n",
        "        t = 1.0\r\n",
        "        action = softmax_sample(visit_counts, actions, t)\r\n",
        "    elif mode == 'max':\r\n",
        "        action, _ = max(node.children.items(), key=lambda item: item[1].visit_count)\r\n",
        "    counts_exp = np.exp(visit_counts)\r\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)    \r\n",
        "    #return action ,probs,node.value()\r\n",
        "    return action ,np.array(visit_counts)/sum(visit_counts),node.value()\r\n",
        "\r\n",
        "def run_mcts(net, state,prob,root_value,num_simulations,discount = 0.9):\r\n",
        "    \"\"\"\r\n",
        "    Core Monte Carlo Tree Search algorithm.\r\n",
        "    To decide on an action, we run N simulations, always starting at the root of\r\n",
        "    the search tree and traversing the tree according to the UCB formula until we\r\n",
        "    reach a leaf node.\r\n",
        "    \"\"\"\r\n",
        "    prob, root_value = prob.tolist()[0] ,catts(root_value.numpy().ravel())\r\n",
        "    to_play = True\r\n",
        "    action_space=[ i for i in range(len(prob))]#history.action_space()\r\n",
        "    #print(\"action space\",action_space)\r\n",
        "    root = Node(0)\r\n",
        "    expand_node(root, to_play,action_space,state,0.0,prob)#node, to_play, actions_space ,hidden_state,reward,policy\r\n",
        "    add_exploration_noise( root)\r\n",
        "\r\n",
        "\r\n",
        "    min_max_stats = MinMaxStats()\r\n",
        "\r\n",
        "    for _ in range(num_simulations): \r\n",
        "        node = root\r\n",
        "        search_path = [node]\r\n",
        "\r\n",
        "        while node.expanded():\r\n",
        "            action, node = select_child( node, min_max_stats)\r\n",
        "            search_path.append(node)\r\n",
        "\r\n",
        "        # Inside the search tree we use the dynamics function to obtain the next\r\n",
        "        # hidden state given an action and the previous hidden state.\r\n",
        "        parent = search_path[-2]\r\n",
        "        \r\n",
        "        #network_output = network.recurrent_inference(parent.hidden_state, action)\r\n",
        "        next_state,r,action_probs, value = dynamics(net,parent.hidden_state,onehot(action,len(action_space))) \r\n",
        "        expand_node(node, to_play, action_space,next_state,r,action_probs)#node, to_play, actions_space ,hidden_state,reward,policy\r\n",
        "\r\n",
        "        backpropagate(search_path, value, to_play, discount, min_max_stats)#search_path, value,,discount, min_max_stats\r\n",
        "    return root    \r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-NpN4lU12kW"
      },
      "source": [
        "import gym\r\n",
        "class ScalingObservationWrapper(gym.ObservationWrapper):\r\n",
        "    \"\"\"\r\n",
        "    Wrapper that apply a min-max scaling of observations.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, env, low=None, high=None):\r\n",
        "        super().__init__(env)\r\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\r\n",
        "\r\n",
        "        low = np.array(self.observation_space.low if low is None else low)\r\n",
        "        high = np.array(self.observation_space.high if high is None else high)\r\n",
        "\r\n",
        "        self.mean = (high + low) / 2\r\n",
        "        self.max = high - self.mean\r\n",
        "\r\n",
        "    def observation(self, observation):\r\n",
        "        return (observation - self.mean) / self.max"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVGfWytBB1"
      },
      "source": [
        "\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from tqdm import tqdm\r\n",
        "def onehot(a,n=2):\r\n",
        "  return np.eye(n)[a]\r\n",
        "def play_game(env,net,n_sim,discount,render):\r\n",
        "    trajectory=[]\r\n",
        "    state = env.reset() \r\n",
        "    done = False\r\n",
        "    while not done:\r\n",
        "        if render:\r\n",
        "          env.render()\r\n",
        "        h ,prob,value= net.inference_initial_state(torch.tensor([state]).float()) \r\n",
        "        root  = run_mcts(net,h,prob,value,num_simulations=n_sim,discount=discount)\r\n",
        "        action,action_prob,mcts_val = select_action(root)\r\n",
        "        value=mcts_val \r\n",
        "        next_state, reward, done, info = env.step(action)\r\n",
        "        data = (state,onehot(action),action_prob,mcts_val,reward)\r\n",
        "        trajectory.append(data)\r\n",
        "        state = next_state\r\n",
        "    print(\"played for \",len(trajectory),\" steps\")   \r\n",
        "    return trajectory    \r\n",
        "\r\n",
        "def sample_games(buffer,batch_size):\r\n",
        "    # Sample game from buffer either uniformly or according to some priority\r\n",
        "    #print(\"samplig from .\",len(buffer))\r\n",
        "    return random.choices(buffer, k=batch_size)\r\n",
        "\r\n",
        "def sample_position(trajectory):\r\n",
        "    # Sample position from game either uniformly or according to some priority.\r\n",
        "    return np.random.randint(0, len(trajectory))\r\n",
        "\r\n",
        "\r\n",
        "def sample_batch(action_space_size,buffer,discount,batch_size,num_unroll_steps, td_steps):\r\n",
        "    obs_batch, action_batch, reward_batch, value_batch, policy_batch = [], [], [], [], []\r\n",
        "    games = sample_games(buffer,batch_size)\r\n",
        "    for g in games:\r\n",
        "      game_pos = sample_position(g)#state index\r\n",
        "      state,action,action_prob,root_val,reward = zip(*g)\r\n",
        "      state,action,action_prob,root_val,reward =list(state),list(action),list(action_prob),list(root_val),list(reward)\r\n",
        "\r\n",
        "      _actions = action[game_pos:game_pos + num_unroll_steps]\r\n",
        "      # random action selection to complete num_unroll_steps\r\n",
        "      _actions += [onehot(np.random.randint(0, action_space_size))for _ in range(num_unroll_steps - len(_actions))]\r\n",
        "\r\n",
        "      obs_batch.append(state[game_pos])\r\n",
        "      action_batch.append(_actions)\r\n",
        "      value, reward, policy = make_target(child_visits=action_prob ,root_values=root_val,rewards=reward,state_index=game_pos,discount=discount, num_unroll_steps=num_unroll_steps, td_steps=td_steps)\r\n",
        "      reward_batch.append(reward)\r\n",
        "      value_batch.append(value)\r\n",
        "      policy_batch.append(policy)\r\n",
        "\r\n",
        "    obs_batch = torch.tensor(obs_batch).float()\r\n",
        "    action_batch = torch.tensor(action_batch).long()\r\n",
        "    reward_batch = torch.tensor(reward_batch).float()\r\n",
        "    value_batch = torch.tensor(value_batch).float()\r\n",
        "    policy_batch = torch.tensor(policy_batch).float()\r\n",
        "    return obs_batch, action_batch, reward_batch, value_batch, policy_batch\r\n",
        "\r\n",
        "\r\n",
        "def make_target(child_visits,root_values,rewards,state_index,discount=0.99, num_unroll_steps=5, td_steps=10):\r\n",
        "        # The value target is the discounted root value of the search tree N steps into the future, plus\r\n",
        "        # the discounted sum of all rewards until then.\r\n",
        "        target_values, target_rewards, target_policies = [], [], []\r\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\r\n",
        "            bootstrap_index = current_index + td_steps\r\n",
        "            if bootstrap_index < len(root_values):\r\n",
        "                value = root_values[bootstrap_index] * discount ** td_steps\r\n",
        "            else:\r\n",
        "                value = 0\r\n",
        "\r\n",
        "            for i, reward in enumerate(rewards[current_index:bootstrap_index]):\r\n",
        "                value += reward * discount ** i\r\n",
        "\r\n",
        "            if current_index < len(root_values):\r\n",
        "                target_values.append(stcat(value))\r\n",
        "                target_rewards.append(stcat(rewards[current_index]))\r\n",
        "                target_policies.append(child_visits[current_index])\r\n",
        "\r\n",
        "            else:\r\n",
        "                # States past the end of games are treated as absorbing states.\r\n",
        "                target_values.append(stcat(0))\r\n",
        "                target_rewards.append(stcat(0))\r\n",
        "                # Note: Target policy is  set to 0 so that no policy loss is calculated for them\r\n",
        "                #target_policies.append([0 for _ in range(len(child_visits[0]))])\r\n",
        "                target_policies.append(child_visits[0]*0.0)\r\n",
        "\r\n",
        "        return target_values, target_rewards, target_policies\r\n",
        "\r\n",
        "\r\n",
        "def scalar_reward_loss( prediction, target):\r\n",
        "        return -(torch.log(prediction) * target).sum(1)\r\n",
        "\r\n",
        "def scalar_value_loss( prediction, target):\r\n",
        "        return -(torch.log(prediction) * target).sum(1)\r\n",
        "def update_weights(model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps ):\r\n",
        "    batch = sample_batch(action_space_size,replay_buffer,discount,batch_size,num_unroll_steps, td_steps)\r\n",
        "    obs_batch, action_batch, target_reward, target_value, target_policy = batch\r\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "    obs_batch = obs_batch.to(device)\r\n",
        "    action_batch = action_batch.to(device)#.unsqueeze(-1) # its not onehot yet \r\n",
        "    target_reward = target_reward.to(device)\r\n",
        "    target_value = target_value.to(device)\r\n",
        "    target_policy = target_policy.to(device)\r\n",
        "\r\n",
        "    # transform targets to categorical representation # its already done\r\n",
        "    # Reference:  Appendix F\r\n",
        "    #transformed_target_reward = config.scalar_transform(target_reward)\r\n",
        "    target_reward_phi =target_reward #config.reward_phi(transformed_target_reward)\r\n",
        "    #transformed_target_value = config.scalar_transform(target_value)\r\n",
        "    target_value_phi = target_value#config.value_phi(transformed_target_value)\r\n",
        "\r\n",
        "    hidden_state, policy_prob,value  = model.initial_state(obs_batch) # initial model_call ###################################### make changes\r\n",
        "    #h,init_pred_p,init_pred_v = net.initial_state(in_s)\r\n",
        "\r\n",
        "    value_loss = scalar_value_loss(value, target_value_phi[:, 0])\r\n",
        "    policy_loss = -(torch.log(policy_prob) * target_policy[:, 0]).sum(1)\r\n",
        "    reward_loss = torch.zeros(batch_size, device=device)\r\n",
        "\r\n",
        "    gradient_scale = 1 / num_unroll_steps\r\n",
        "    for step_i in range(num_unroll_steps):\r\n",
        "        hidden_state, reward,policy_prob,value  = model.next_state(hidden_state, action_batch[:, step_i]) ######################### make changes\r\n",
        "        #h,pred_reward,pred_policy,pred_value= net.next_state(h,act)\r\n",
        "        policy_loss += -(torch.log(policy_prob) * target_policy[:, step_i + 1]).sum(1)\r\n",
        "        value_loss += scalar_value_loss(value, target_value_phi[:, step_i + 1])\r\n",
        "        reward_loss += scalar_reward_loss(reward, target_reward_phi[:, step_i])\r\n",
        "        hidden_state.register_hook(lambda grad: grad * 0.5)\r\n",
        "\r\n",
        "    # optimize\r\n",
        "    value_loss_coeff = 1\r\n",
        "    loss = (policy_loss + value_loss_coeff * value_loss + reward_loss) # find value loss coefficiet = 1?\r\n",
        "    weights = 1\r\n",
        "    weighted_loss = (weights * loss).mean()#1?\r\n",
        "    weighted_loss.register_hook(lambda grad: grad * gradient_scale)\r\n",
        "    loss = loss.mean()\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    weighted_loss.backward()\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)#5?\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "def adjust_lr(optimizer, step_count):\r\n",
        "\r\n",
        "    lr_init=0.05\r\n",
        "    lr_decay_rate=0.01\r\n",
        "    lr_decay_steps=10000\r\n",
        "    lr = lr_init * lr_decay_rate ** (step_count / lr_decay_steps)\r\n",
        "    lr = max(lr, 0.001)\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        param_group['lr'] = lr\r\n",
        "    return lr\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "def net_train(net,  action_space_size, replay_buffer,discount,batch_size,num_unroll_steps, td_steps):\r\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "    model =MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5) #net\r\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9,weight_decay=1e-4)\r\n",
        "    training_steps=5000#20000\r\n",
        "    # wait for replay buffer to be non-empty\r\n",
        "    while len(replay_buffer) == 0:\r\n",
        "        pass\r\n",
        "\r\n",
        "    for step_count in tqdm(range(training_steps)):\r\n",
        "        lr = adjust_lr( optimizer, step_count)\r\n",
        "        update_weights(model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps)\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KZWra51wFVvb",
        "outputId": "e3b47c88-c975-43b1-8ab9-1f2b75b1e6b3"
      },
      "source": [
        "import gym\r\n",
        "import numpy as np\r\n",
        "buffer =[]\r\n",
        "\r\n",
        "render = False\r\n",
        "episodes_per_train=20\r\n",
        "training_steps=50\r\n",
        "epochs=50\r\n",
        "n_sim= 15\r\n",
        "discount = 0.99\r\n",
        "batch_size = 128\r\n",
        "envs = ['CartPole-v1']\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "env=gym.make(envs[0])\r\n",
        "env = ScalingObservationWrapper(env, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5])\r\n",
        "#env=env.unwrapped\r\n",
        "\r\n",
        "s_dim =env.observation_space.shape[0]\r\n",
        "print(\"s_dim: \",s_dim)\r\n",
        "a_dim =env.action_space.n\r\n",
        "print(\"a_dim: \",a_dim)\r\n",
        "a_bound =1 #env.action_space.high[0]\r\n",
        "print(\"a_bound: \",a_bound)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "net = MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5)\r\n",
        "\r\n",
        "for t in range(training_steps):\r\n",
        "  for _ in range(episodes_per_train):\r\n",
        "    buffer.append(play_game(env,net,n_sim,discount,render))\r\n",
        "  print(\"training\")  \r\n",
        "  net = net_train(net,  action_space_size=a_dim, replay_buffer=buffer,discount=discount,batch_size=batch_size,num_unroll_steps=5, td_steps=10)\r\n",
        "  \r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s_dim:  4\n",
            "a_dim:  2\n",
            "a_bound:  1\n",
            "played for  15  steps\n",
            "played for  34  steps\n",
            "played for  23  steps\n",
            "played for  34  steps\n",
            "played for  39  steps\n",
            "played for  10  steps\n",
            "played for  46  steps\n",
            "played for  37  steps\n",
            "played for  13  steps\n",
            "played for  13  steps\n",
            "played for  13  steps\n",
            "played for  16  steps\n",
            "played for  34  steps\n",
            "played for  15  steps\n",
            "played for  20  steps\n",
            "played for  43  steps\n",
            "played for  40  steps\n",
            "played for  12  steps\n",
            "played for  58  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/5000 [00:00<06:43, 12.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "played for  26  steps\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [06:36<00:00, 12.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "played for  393  steps\n",
            "played for  500  steps\n",
            "played for  500  steps\n",
            "played for  500  steps\n",
            "played for  433  steps\n",
            "played for  499  steps\n",
            "played for  500  steps\n",
            "played for  373  steps\n",
            "played for  324  steps\n",
            "played for  284  steps\n",
            "played for  500  steps\n",
            "played for  500  steps\n",
            "played for  457  steps\n",
            "played for  500  steps\n",
            "played for  453  steps\n",
            "played for  309  steps\n",
            "played for  500  steps\n",
            "played for  457  steps\n",
            "played for  436  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/5000 [00:00<06:59, 11.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "played for  500  steps\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [07:09<00:00, 11.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "played for  400  steps\n",
            "played for  500  steps\n",
            "played for  463  steps\n",
            "played for  448  steps\n",
            "played for  494  steps\n",
            "played for  500  steps\n",
            "played for  438  steps\n",
            "played for  500  steps\n",
            "played for  500  steps\n",
            "played for  438  steps\n",
            "played for  420  steps\n",
            "played for  500  steps\n",
            "played for  500  steps\n",
            "played for  500  steps\n",
            "played for  500  steps\n",
            "played for  500  steps\n",
            "played for  500  steps\n",
            "played for  462  steps\n",
            "played for  500  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/5000 [00:00<07:03, 11.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "played for  456  steps\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 1077/5000 [01:34<05:37, 11.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4c6891da48c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_sim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0maction_space_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_unroll_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2626ebf13f47>\u001b[0m in \u001b[0;36mnet_train\u001b[0;34m(net, action_space_size, replay_buffer, discount, batch_size, num_unroll_steps, td_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep_count\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjust_lr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_unroll_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2626ebf13f47>\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(model, action_space_size, optimizer, replay_buffer, discount, batch_size, num_unroll_steps, td_steps)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_unroll_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_steps\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_unroll_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mobs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2626ebf13f47>\u001b[0m in \u001b[0;36msample_batch\u001b[0;34m(action_space_size, buffer, discount, batch_size, num_unroll_steps, td_steps)\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mobs_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgame_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0maction_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_visits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_prob\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mroot_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgame_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_unroll_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_unroll_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtd_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m       \u001b[0mreward_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0mvalue_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2626ebf13f47>\u001b[0m in \u001b[0;36mmake_target\u001b[0;34m(child_visits, root_values, rewards, state_index, discount, num_unroll_steps, td_steps)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mtarget_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mtarget_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mtarget_policies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_visits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c20c725e4feb>\u001b[0m in \u001b[0;36mstcat\u001b[0;34m(x, support)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mfloor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfloor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msupport\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mfirst_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0msecond_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4QeRNKzVSoj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}