{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_LSTM_LUNAR_LANDER_CONTINUOUS_POMP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKhictAAyYliJTpSFE/DCG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NonMagneticNeedle/reinforcement_learning/blob/master/pomdp/LSTM-PPO%20TF.KERAS/PPO_LSTM_LUNAR_LANDER_CONTINUOUS_POMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZG1eqIS3OoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDvssQd64Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5esgX013vPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqOVgu2CWjx3",
        "colab_type": "code",
        "outputId": "64453941-9845-440b-dbce-e868cfb5bdbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "!pip install gym[all]\n",
        "!pip install box2d-py\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "\n",
        "# Special gym environment\n",
        "#!pip install gym[atari]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.18.4)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[all]) (1.4.1)\n",
            "Requirement already satisfied: imageio; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (2.4.1)\n",
            "Collecting box2d-py~=2.3.5; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 2.7MB/s \n",
            "\u001b[?25hCollecting mujoco-py<2.0,>=1.50; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"all\" in /usr/local/lib/python3.6/dist-packages (from gym[all]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\n",
            "Collecting glfw>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/0d/aff94f4258dd21556121802e0274a6b92bf5e925b37898abb961b8924025/glfw-1.11.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.29.17)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.14.0)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/22/9460e311f340cb62d26a38c419b1381b8593b0bb6b5d1f056938b086d362/lockfile-0.12.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.20)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: box2d-py, glfw, lockfile, mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-49ny38ym/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-49ny38ym/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-tp07pnm1/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvwBsGWG4h6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "80fc0725-8992-4fee-9a34-52cd44a96594"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%cd /gdrive/My Drive/ll_clean"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/ll_clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-_0PnKJHH3X",
        "colab_type": "code",
        "outputId": "85187cde-1bae-474e-b842-ed81a389d5c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "import math\n",
        "import time\n",
        "#import tensorflow_probability as tfp\n",
        "#display gym\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\"\"\"\"\"\"\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "class ppo():\n",
        "\tdef __init__(self,name,s_dim,a_dim,memory,a_bound,time_steps,lstm_units):\n",
        "\t\tself.s_dim = s_dim\n",
        "\t\tself.a_dim =a_dim\n",
        "\t\tself.memory = memory\n",
        "\t\tself.a_bound =a_bound\n",
        "\t\tself.time_steps =time_steps\n",
        "\t\tself.name = name\n",
        "\t\tself.lstm_units= lstm_units\n",
        "\t\tself.actor  = self.make_actor()\n",
        "\t\tself.critic  = self.make_critic()\n",
        "\t\t\n",
        "\tdef make_actor(self):\n",
        "\t\t\t\n",
        "\n",
        "\t\tstate_inputs = tf.keras.Input(batch_shape=(1,self.time_steps,self.s_dim), name='state')\n",
        "\t\tadvantage = tf.keras.Input(batch_shape=(1,self.time_steps,1 ), name=\"Advantage\")\n",
        "\t\taction= tf.keras.Input(batch_shape=(1,self.time_steps,self.a_dim), name=\"action\")\n",
        "\t\tx = tf.keras.layers.Masking(mask_value=-1.)(state_inputs)\t\n",
        "\t\tx = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(200, activation='relu'))(x)\n",
        "\t\tx,state_h, state_c = tf.keras.layers.LSTM(units= self.lstm_units,return_sequences=True,return_state=True,stateful=True,name='a_lstm')(x)\n",
        "\n",
        "\t\tx = tf.keras.layers.Dense(200, activation='relu')(x)\n",
        "\t\t\n",
        "\t\tmu_0 = tf.keras.layers.Dense(self.a_dim, activation='tanh')(x)\n",
        "\t\tsigma_0 = tf.keras.layers.Dense(self.a_dim, activation='relu')(x)\n",
        "\t\tmu = tf.keras.layers.Lambda(lambda x:tf.clip_by_value(x * self.a_bound,-self.a_bound ,self.a_bound) )(mu_0)\n",
        "\t\tcovari = tf.keras.layers.Lambda(lambda x:tf.clip_by_value(x,1e-2 , 1e+02))(sigma_0)\n",
        "\t\tmucov=tf.keras.layers.concatenate([mu,covari],axis=-1,name = \"policy_head\")\n",
        "\t\t\n",
        "\t\tdef proximal_policy_optimization_loss(advantage, action):\n",
        "\t\t\tloss_clipping = 0.2\n",
        "\t\t\tentropy_loss = 0.001\n",
        "\t\t\tpi=3.1415926\n",
        "\t\t\tk=self.a_dim\n",
        "\t\t\taction =action[0]\n",
        "\t\t\tadvantage = advantage[0]\n",
        "\t\t\tdef loss(y_true,y_pred):\n",
        "\n",
        "\t\t\t\t#print(\"shapes\",tf.keras.backend.int_shape(y_true))\n",
        "\t\t\t\tmu =y_pred[0,:,:k]#batch,timesteps,k\n",
        "\t\t\t\tcov = y_pred[0,:,k:]#batch,timesteps,k\n",
        "\t\t\t\t#tf.print(tf.keras.backend.int_shape(mu),tf.keras.backend.int_shape(cov))\n",
        "\t\t\t\told_mu = y_true[0,:,:k]\n",
        "\t\t\t\told_cov = y_true[0,:,k:]\n",
        "\n",
        "\t\t\t\tx= action\n",
        "\n",
        "\t\t\t\tdet = tf.keras.backend.prod(cov,axis=1,keepdims=True)\n",
        "\t\t\t\tinv = 1/cov#tf.linalg.inv(cov_mat)\n",
        "\t\t\t\tnorm_const = 1.0/ ( tf.keras.backend.pow(2*pi,k/2) * tf.keras.backend.pow(det,1.0/2) )\n",
        "\t\t\t\tx_mu = x - mu\n",
        "\t\t\t\tx_mu_sq = tf.keras.backend.square(x_mu)\t\n",
        "\t\t\t\tprod  = inv*x_mu_sq\n",
        "\t\t\t\t\n",
        "\t\t\t\tprod2 =tf.keras.backend.sum(prod,axis=1,keepdims=True) \n",
        "\t\t\t\tresult = tf.keras.backend.exp( -0.5 *prod2)\n",
        "\t\t\t\tpdf = norm_const * result\n",
        "\t\t\t\t\n",
        "\t\t\t\told_det = tf.keras.backend.prod(old_cov,axis=1,keepdims=True)\n",
        "\t\t\t\told_inv = 1/old_cov#tf.linalg.old_inv(old_cov_mat)\n",
        "\t\t\t\told_norm_const = 1.0/ ( tf.keras.backend.pow(2*pi,k/2) * tf.keras.backend.pow(old_det,1.0/2) )\n",
        "\t\t\t\told_x_mu = x - old_mu\n",
        "\t\t\t\told_x_mu_sq = tf.keras.backend.square(old_x_mu)\t\n",
        "\t\t\t\told_prod  = old_inv*old_x_mu_sq\n",
        "\t\t\t\told_prod2 =tf.keras.backend.sum(old_prod,axis=1,keepdims=True) \n",
        "\t\t\t\told_result = tf.keras.backend.exp( -0.5 *old_prod2)\n",
        "\t\t\t\told_pdf = old_norm_const * old_result\t\t\t\t\t\n",
        "\t\t\t\tlog_pdf = tf.keras.backend.log(pdf + tf.keras.backend.epsilon())\n",
        "\t\t\t\told_log_pdf = tf.keras.backend.log(old_pdf + tf.keras.backend.epsilon() )\n",
        "\t\t\t\tentropy =  tf.keras.backend.sum(0.5 * (tf.keras.backend.log(2. * pi * det) + 1.))\n",
        "\t\t\t\tr = tf.keras.backend.exp(log_pdf- old_log_pdf)\n",
        "\t\t\t\tloss = -tf.keras.backend.mean(tf.keras.backend.minimum(r * advantage, tf.keras.backend.clip(r, min_value=1 - loss_clipping,max_value=1 + loss_clipping) * advantage)) #+ entropy_loss *entropy\n",
        "\t\t\t\t#loss = -tf.keras.backend.minimum(r * advantage, tf.keras.backend.clip(r, min_value=1 - loss_clipping,max_value=1 + loss_clipping) * advantage) #+ entropy_loss *entropy\n",
        "\t\t\t\t\n",
        "\t\t\t\treturn loss\n",
        "\t\t\treturn loss\n",
        "\n",
        "\t\tactor_model= tf.keras.Model(inputs=[state_inputs, advantage,action], outputs=[mucov,state_h,state_c], name='actor_model')\n",
        "\t\tactor_model.compile(loss={'policy_head':proximal_policy_optimization_loss(advantage=advantage,action=action)} ,optimizer=tf.keras.optimizers.Adam(lr=0.00001))\n",
        "\t\treturn actor_model\n",
        "\tdef make_critic(self):\n",
        "\t\t\t\n",
        "\n",
        "\t\tstate_inputs = tf.keras.Input(batch_shape=(1,self.time_steps,self.s_dim), name='state')\n",
        "\t\t#mask_value = tf.keras.Input(batch_shape=(1,self.time_steps,1 ), name=\"Mask_value\")\n",
        "\t\tx = tf.keras.layers.Masking(mask_value=-1.)(state_inputs)\t\n",
        "\t\tx = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(200, activation='relu'))(x)\n",
        "\t\tx,state_h, state_c = tf.keras.layers.LSTM(units= self.lstm_units,return_sequences=True,return_state=True,stateful=True,name='c_lstm')(x)\n",
        "\n",
        "\t\tx = tf.keras.layers.Dense(200, activation='relu',use_bias=True)(x)\n",
        "\t\t\n",
        "\t\tval = tf.keras.layers.Dense(1, activation=None,name = \"value_head\",use_bias=True)(x)\n",
        "\t\t#hope is msaking makes skips the gradients too # still it trains the layers (after lstms) bias value even if the loss is 0 due to mask\n",
        "\t\tcritic_model= tf.keras.Model(inputs=state_inputs, outputs=[val,state_h,state_c], name='critic_model')\n",
        "\t\tcritic_model.compile(loss={'value_head':'mse'} ,optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
        "\t\treturn critic_model\n",
        "\n",
        "\n",
        "\tdef save_weights(self):\n",
        "\t\tpath=r\"C:\\\\Users\\\\Dell\\\\Desktop\\\\this_year\\\\SCM-RL\\\\ppo_continous_\"+self.name+\"model.h5\"\n",
        "\t\tself.model.save_weights(path)\n",
        "\t\tprint(\"saved\")\n",
        "\tdef load_weights(self):\n",
        "\t\tpath=r\"C:\\\\Users\\\\Dell\\\\Desktop\\\\this_year\\\\SCM-RL\\\\ppo_continous_\"+self.name+\"model.h5\"\n",
        "\t\tself.model.load_weights(path)\n",
        "\t\tprint(\"loaded\")\n",
        "\tdef set_lstm_state_actor(self,h,c):\n",
        "\t\tself.actor.get_layer('a_lstm').reset_states([h,c])\n",
        "\tdef zero_lstm_state_actor(self):\n",
        "\t\tc_0 = np.zeros((1,self.lstm_units)).astype(np.float32)\n",
        "\t\th_0 = np.zeros((1,self.lstm_units)).astype(np.float32)\n",
        "\t\tself.actor.get_layer('a_lstm').reset_states([h_0,c_0])\n",
        "\n",
        "\tdef set_lstm_state_critic(self,h,c):\n",
        "\t\tself.critic.get_layer('a_lstm').reset_states([h,c])\n",
        "\tdef zero_lstm_state_critic(self):\n",
        "\t\tc_0 = np.zeros((1,self.lstm_units)).astype(np.float32)\n",
        "\t\th_0 = np.zeros((1,self.lstm_units)).astype(np.float32)\n",
        "\t\tself.critic.get_layer('c_lstm').reset_states([h_0,c_0])\n",
        "\t\t\t\n",
        "\tdef gae_calc(self,val,val_,rew,done):\n",
        "\t\tmask=1 \n",
        "\t\tgae=0\n",
        "\t\tgamma=0.99\n",
        "\t\tlambd = 0.95\n",
        "\t\treturns=np.zeros_like(val)\n",
        "\t\tfor i in reversed(range(0,len(val))):\n",
        "\t\t\tmask=1\n",
        "\t\t\tif done[i]:\n",
        "\t\t\t\tmask = 0 \t\n",
        "\t\t\tdelta=rew[i]+gamma*val_[i]*mask - val[i]\n",
        "\t\t\tgae=delta+gamma*lambd*mask*gae\n",
        "\t\t\treturns[i]=gae+val[i]\n",
        "\t\treturn returns\n",
        "\t\t\n",
        "\tdef adv_calc(self,val,val_,rew,done):\n",
        "\t\tgamma=0.99\n",
        "\t\treturns=np.zeros_like(val)\n",
        "\t\tfor i in range(0,len(val)):\n",
        "\t\t\treturns[i] = rew[i] + (1- done[i])*val_[i]*gamma\n",
        "\t\treturn returns\n",
        "\tdef batchify(self,lst):\n",
        "\t\tdef ceil(n):\n",
        "\t\t\tres = int(n)\n",
        "\t\t\treturn res if res == n or n < 0 else res+1\t\n",
        "\t\tdef front_pad_batch(batchsize,data):\n",
        "\t\t\td = np.zeros((batchsize,data.shape[-1]))-1.\n",
        "\t\t\td[batchsize-data.shape[0]:] = data\n",
        "\t\t\treturn d\t\n",
        "\t\tbatchsize =self.time_steps\n",
        "\t\tfull_size = ceil(lst.shape[0]/batchsize)*batchsize\n",
        "\n",
        "\t\treturn front_pad_batch(full_size,lst)\t\n",
        "\tdef batchify_adv(self,lst):\n",
        "\t\tdef ceil(n):\n",
        "\t\t\tres = int(n)\n",
        "\t\t\treturn res if res == n or n < 0 else res+1\t\n",
        "\t\tdef front_pad_batch(batchsize,data):\n",
        "\t\t\td = np.zeros((batchsize,data.shape[-1]))\n",
        "\t\t\td[batchsize-data.shape[0]:] = data\n",
        "\t\t\treturn d\t\n",
        "\t\tbatchsize =self.time_steps\n",
        "\t\tfull_size = ceil(lst.shape[0]/batchsize)*batchsize\n",
        "\n",
        "\t\treturn front_pad_batch(full_size,lst)\t\t\t\n",
        "\tdef batchify_musig(self,lst):\n",
        "\t\tdef ceil(n):\n",
        "\t\t\tres = int(n)\n",
        "\t\t\treturn res if res == n or n < 0 else res+1\t\n",
        "\t\tdef front_pad_batch(batchsize,data):\n",
        "\t\t\td = np.zeros((batchsize,data.shape[-1]))+1\n",
        "\t\t\td[batchsize-data.shape[0]:] = data\n",
        "\t\t\treturn d\t\n",
        "\t\tbatchsize =self.time_steps\n",
        "\t\tfull_size = ceil(lst.shape[0]/batchsize)*batchsize\n",
        "\n",
        "\t\treturn front_pad_batch(full_size,lst)\t\n",
        "\tdef train(self,epochs=10):\n",
        "\t\ttime_horizon=self.time_steps\n",
        "\n",
        "\t\tobs =np.array(self.memory.batch_s)\n",
        "\t\tobs_ =np.array(self.memory.batch_s_)\n",
        "\t\ths =np.array(self.memory.batch_h)\t\n",
        "\t\tcs =np.array(self.memory.batch_c)\n",
        "\t\tall_obs=self.memory.batch_s.copy()\n",
        "\t\tall_obs.append(self.memory.batch_s_[-1]  )   \n",
        "\t\tall_obs = np.array(all_obs)\n",
        "\t\tall_obs_no = all_obs.shape[0]    \n",
        "\t\tall_obs_batch = self.batchify(all_obs)               \n",
        "\t\tall_values = np.zeros((all_obs_batch.shape[0],1))\n",
        "\t\tself.zero_lstm_state_critic()    \n",
        "\t\tfor b in range(0,all_obs_batch.shape[0],time_horizon):\n",
        "\t\t\tvb ,h,c=\tself.critic.predict(x=(np.array([all_obs_batch[b:b+time_horizon]])))\n",
        "\t\t\tall_values[b:b+time_horizon] = vb[0]\n",
        "\t\t\t\n",
        "\t\tvalues =all_values[-all_obs_no:-1] \n",
        "\t\tvalues_ = all_values[-all_obs_no+1:]\t\n",
        "\t\treturns = self.gae_calc(values,values_,self.memory.batch_r,self.memory.batch_done)\n",
        "\t\tadvantage=np.array(returns-values)\n",
        "\t\tAction=np.array(self.memory.batch_a)\n",
        "\t\tOld_Prediction_musig =np.array(self.memory.musig)\n",
        "\t\tOld_cov = np.array(self.memory.cov)\n",
        "\t\t#print(obs.shape,advantage.shape,Action.shape,Old_Prediction_musig.shape,returns.shape)\t\t\n",
        "\t\tobs = self.batchify(obs)\n",
        "\t\tadvantage = self.batchify_adv(advantage)\n",
        "\t\tAction =self.batchify_adv(Action)\n",
        "\t\tOld_Prediction_musig =self.batchify_musig(Old_Prediction_musig)\n",
        "\t\treturns =self.batchify(returns)\n",
        "\t\t#print(obs.shape,advantage.shape,Action.shape,Old_Prediction_musig.shape,returns.shape)\n",
        "\t\t#print(returns,obs)\n",
        "\t\tfor ep in range(epochs):\n",
        "\t\t\tself.zero_lstm_state_actor()\n",
        "\t\t\tself.zero_lstm_state_critic()\n",
        "\t\t\tfor b in range(0,obs.shape[0],time_horizon):\n",
        "\t\t\t\t#train actor\n",
        "\t\t\t\tself.actor.fit(x=(np.array([obs[b:b+time_horizon]]),np.array([advantage[b:b+time_horizon]]),np.array([Action[b:b+time_horizon]])),y={'policy_head':np.array([Old_Prediction_musig[b:b+time_horizon]])},shuffle=False, epochs=1, verbose=0)\n",
        "\n",
        "\t\t\t\t#train critic\t\n",
        "\t\t\t\tself.critic.fit(x=(np.array([obs[b:b+time_horizon]])),y={'value_head':np.array([returns[b:b+time_horizon]])},shuffle=False, epochs=1, verbose=0)\n",
        "\t\tself.memory.clear()\n",
        "\n",
        "\tdef train_crazy(self,epochs=10):\n",
        "\t\tfor ep in range(epochs):\n",
        "\t\t\ttime_horizon=self.time_steps\n",
        "\n",
        "\t\t\tobs =np.array(self.memory.batch_s)\n",
        "\t\t\tobs_ =np.array(self.memory.batch_s_)\n",
        "\t\t\ths =np.array(self.memory.batch_h)\t\n",
        "\t\t\tcs =np.array(self.memory.batch_c)\n",
        "\t\t\tall_obs=self.memory.batch_s.copy()\n",
        "\t\t\tall_obs.append(self.memory.batch_s_[-1]  )   \n",
        "\t\t\tall_obs = np.array(all_obs)\n",
        "\t\t\tall_obs_no = all_obs.shape[0]    \n",
        "\t\t\tall_obs_batch = self.batchify(all_obs)               \n",
        "\t\t\tall_values = np.zeros((all_obs_batch.shape[0],1))\n",
        "\t\t\tself.zero_lstm_state_critic()    \n",
        "\t\t\tfor b in range(0,all_obs_batch.shape[0],time_horizon):\n",
        "\t\t\t\tvb ,h,c=\tself.critic.predict(x=(np.array([all_obs_batch[b:b+time_horizon]])))\n",
        "\t\t\t\tall_values[b:b+time_horizon] = vb[0]\n",
        "\t\t\t\t\n",
        "\t\t\tvalues =all_values[-all_obs_no:-1] \n",
        "\t\t\tvalues_ = all_values[-all_obs_no+1:]\t\n",
        "\t\t\treturns = self.gae_calc(values,values_,self.memory.batch_r,self.memory.batch_done)\n",
        "\t\t\tadvantage=np.array(returns-values)\n",
        "\t\t\tAction=np.array(self.memory.batch_a)\n",
        "\t\t\tOld_Prediction_musig =np.array(self.memory.musig)\n",
        "\t\t\tOld_cov = np.array(self.memory.cov)\n",
        "\t\t\t#print(obs.shape,advantage.shape,Action.shape,Old_Prediction_musig.shape,returns.shape)\t\t\n",
        "\t\t\tobs = self.batchify(obs)\n",
        "\t\t\tadvantage = self.batchify_adv(advantage)\n",
        "\t\t\tAction =self.batchify_adv(Action)\n",
        "\t\t\tOld_Prediction_musig =self.batchify_musig(Old_Prediction_musig)\n",
        "\t\t\treturns =self.batchify(returns)\n",
        "\t\t\t#print(obs.shape,advantage.shape,Action.shape,Old_Prediction_musig.shape,returns.shape)\n",
        "\t\t\t#print(returns,obs)\n",
        "\t\t\n",
        "\t\t\tself.zero_lstm_state_actor()\n",
        "\t\t\tself.zero_lstm_state_critic()\n",
        "\t\t\tfor b in range(0,obs.shape[0],time_horizon):\n",
        "\t\t\t\t#train actor\n",
        "\t\t\t\tself.actor.fit(x=(np.array([obs[b:b+time_horizon]]),np.array([advantage[b:b+time_horizon]]),np.array([Action[b:b+time_horizon]])),y={'policy_head':np.array([Old_Prediction_musig[b:b+time_horizon]])},shuffle=False, epochs=1, verbose=0)\n",
        "\n",
        "\t\t\t\t#train critic\t\n",
        "\t\t\t\tself.critic.fit(x=(np.array([obs[b:b+time_horizon]])),y={'value_head':np.array([returns[b:b+time_horizon]])},shuffle=False, epochs=1, verbose=0)\n",
        "\t\tself.memory.clear()\n",
        "\n",
        "\n",
        "class Memory:\n",
        "\tdef __init__(self):\n",
        "\t\tself.batch_s = []\n",
        "\t\tself.batch_a = []\n",
        "\t\tself.batch_r = []\n",
        "\t\tself.batch_s_ = []\n",
        "\t\tself.batch_h = []\n",
        "\t\tself.batch_c =[]\n",
        "\t\tself.batch_done = []\n",
        "\t\tself.musig = []\n",
        "\t\tself.cov=[]\n",
        "\tdef store(self, s, a, s_, r, done,musig,cov,h,c):\n",
        "\t\tself.batch_s.append(s)\n",
        "\t\tself.batch_a.append(a)\n",
        "\t\tself.batch_r.append(r)\n",
        "\t\tself.batch_s_.append(s_)\n",
        "\t\tself.batch_done.append(done)\n",
        "\t\tself.musig.append(musig)\n",
        "\t\tself.cov.append(cov)\n",
        "\t\tself.batch_h.append(h)\n",
        "\t\tself.batch_c.append(c)\n",
        "\tdef clear(self):\n",
        "\t\tself.batch_s.clear()\n",
        "\t\tself.batch_a.clear()\n",
        "\t\tself.batch_r.clear()\n",
        "\t\tself.batch_s_.clear()\n",
        "\t\tself.batch_done.clear()\n",
        "\t\tself.musig.clear()\n",
        "\t\tself.cov.clear()\n",
        "\t\tself.batch_h.clear()\n",
        "\t\tself.batch_c.clear()\n",
        "\tdef cnt_samples(self):\n",
        "\t\treturn len(self.batch_s)\n",
        "\n",
        "\n",
        "def front_pad_single(batchsize,data):\n",
        "\td = np.zeros((batchsize,data.shape[-1]))-1.\n",
        "\t#print(\"d \",d.shape ,\"data\",data.shape)\n",
        "\td[-1] = data\n",
        "\t#print(\"padded sequence\",d)\n",
        "\treturn d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "envs = ['Pendulum-v0','LunarLanderContinuous-v2','BipedalWalkerHardcore-v3']\n",
        "\n",
        "masks=[np.array([1., 1., 0.]),np.array([1., 1., 0., 0., 1., 0., 1., 1,]),1]\n",
        "env= envs[1]\n",
        "mask = masks[1]#to make a mdp into pomdp\n",
        "#env1=gym.make('Pendulum-v0')\n",
        "env1=gym.make(env)\n",
        "env1=env1.unwrapped\n",
        "\n",
        "s_dim1 =5# env1.observation_space.shape[0]\n",
        "print(s_dim1)\n",
        "a_dim1 =env1.action_space.shape[0]\n",
        "print(a_dim1)\n",
        "a_bound1 = env1.action_space.high[0]\n",
        "print(a_bound1)\n",
        "\n",
        "memory_1=Memory()\n",
        "time_horizon=64##########################\n",
        "\n",
        "agent_1 =  ppo(name = \"ppo_agent_01\",s_dim=s_dim1 ,a_dim= a_dim1,memory = memory_1,a_bound=a_bound1,time_steps=time_horizon,lstm_units=200)\n",
        "\n",
        "def mask_lander(state):\n",
        "\treturn np.array([state[0],state[1],state[4],state[6],state[7]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DUMMY_ACTION1, DUMMY_VALUE1 ,DUMMY_LOSS_MASK = np.zeros((1,time_horizon,a_dim1)), np.zeros((1,time_horizon, 1)),np.zeros((1,time_horizon, 1))\n",
        "\n",
        "#check if final hidden states are consistnent oor not by taking time as 1,2,5,10 \n",
        "#find vs_ of terminal state\n",
        "episodes = 30000\n",
        "steps = 1000\n",
        "render=0\n",
        "print(\"starting>>>\")\n",
        "\n",
        "\n",
        "\n",
        "apath=\"/gdrive/My Drive/ll_clean/ppo_lstm_clean_mean_normal_actormodel64.h5\"\n",
        "cpath=\"/gdrive/My Drive/ll_clean/ppo_lstm_clean_mean_normal_criticmodel64.h5\"\n",
        "agent_1.actor.load_weights(apath)\n",
        "agent_1.critic.load_weights(cpath)\n",
        "print(\"loaded\")\n",
        "avg =[]\n",
        "t = time.time()\n",
        "best = 0\n",
        "nz = 0 \n",
        "for episode in range(1,episodes):\n",
        "\tdone1=False\n",
        "\tstp=0\n",
        "\ts1=env1.reset()\n",
        "\ts1 = mask_lander(s1)\n",
        "\tagent_1.zero_lstm_state_actor()\n",
        "\t\n",
        "\trews1 = 0\t\n",
        "\tif episode > 500:\n",
        "\t\trender=0\n",
        "\n",
        "\twhile not done1:\n",
        "\t\t#if stp%(steps//10) ==0:\n",
        "\t\t#\tprint(\"+\",end='')\n",
        "\t\tif render:\n",
        "\t\t\tenv1.render()\t\n",
        "\t\toutput1= agent_1.actor.predict((np.array([front_pad_single(time_horizon,s1)]),DUMMY_VALUE1,DUMMY_ACTION1))\n",
        "\t\t#print(output1)\n",
        "\t\tout1 = output1[0][0][-1]# getting the recent action\n",
        "\t\th = output1[1]\n",
        "\t\tc = output1[2]\n",
        "\t\tmu_pred1,cov_pred1 =out1[:a_dim1],out1[a_dim1:]\n",
        "\t\tcov_pred1 = np.diag(cov_pred1)\n",
        "\t\taction1= np.random.multivariate_normal(mu_pred1,cov_pred1,a_dim1)[0]\n",
        "\t\ta1 =np.clip(action1,-a_bound1,a_bound1 )\n",
        "\t\ts_1, reward1, done1, info1 = env1.step(a1)\n",
        "\t\t#print(\"s_1\",s_1, end='')\t\n",
        "\t\ts_1 = mask_lander(s_1)\n",
        "\t\t#print(\" sub:\",s_1-s1)\n",
        "\t\tstp=stp+1\t\n",
        "\n",
        "\n",
        "\t\tif stp>steps:\n",
        "\t\t\tdone1=True\t\t\n",
        "\t\tagent_1.memory.store(s1,action1,s_1,reward1,done1,out1,cov_pred1,h,c)# s, a, s_1, r, done1,musig,h,c\n",
        "\t\trews1+=reward1\n",
        "\n",
        "\t\ts1 = s_1\n",
        "\t# updation\n",
        "\t#print(\"|\")\n",
        "\tif rews1>100:\n",
        "\t\tbest+=1\n",
        "\tif rews1>0:\n",
        "\t\tnz+=1\t\t\n",
        "\tavg.append(rews1)\n",
        "\t#print(\"final_state \",s_1[0],s_1[1],s_1[2],s_1[3],s_1[4])\n",
        "\t#print(env+\" | \"+str(episode)+\" | \"+str(rews1)+\"|\"+str(stp)+\"| pomdpmask:\"+str(mask)+\" |\"+\"time_horizon :\"+str(time_horizon))\n",
        "\t#print(\"_\"*100)\n",
        "\tagent_1.train()\n",
        "\tif episode % 100 == 0:\n",
        "\t\tprint(\"current \",episode , \" average of last 100 episodes :\", np.mean(np.array(avg)), \" max:\", np.max(np.array(avg)), \" min:\", np.min(np.array(avg)),\" best/100 :\" , best, \" games>0 / 100: \",nz)\n",
        "\t\tbest = 0\n",
        "\t\tnz = 0\n",
        "\t\tavg.clear()\n",
        "\n",
        "\t\tapath=\"/gdrive/My Drive/ll_clean/ppo_lstm_clean_mean_normal_actormodel64.h5\"\n",
        "\t\tcpath=\"/gdrive/My Drive/ll_clean/ppo_lstm_clean_mean_normal_criticmodel64.h5\"\n",
        "\n",
        "\t\t#perfect solve around 700th ep\n",
        "\t\tagent_1.actor.save_weights(apath)\n",
        "\t\tagent_1.critic.save_weights(cpath)\n",
        "\t\tprint(\" total time taken (min): \" ,(time.time()-t)/60)\n",
        "\t\tt = time.time()\n",
        "\n",
        "\t\t "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "2\n",
            "1.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Output a_lstm missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to a_lstm.\n",
            "WARNING:tensorflow:Output a_lstm_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to a_lstm_1.\n",
            "WARNING:tensorflow:Output c_lstm missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to c_lstm.\n",
            "WARNING:tensorflow:Output c_lstm_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to c_lstm_1.\n",
            "starting>>>\n",
            "loaded\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:297: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
            "current  100  average of last 100 episodes : -19.61970705060851  max: 258.22680932846737  min: -519.7562228318736  best/100 : 24  games>0 / 100:  39\n",
            " total time taken (min):  14.987387820084889\n",
            "current  200  average of last 100 episodes : -32.28575165179992  max: 261.4011588410707  min: -629.8566063411668  best/100 : 26  games>0 / 100:  37\n",
            " total time taken (min):  17.468558124701183\n",
            "current  300  average of last 100 episodes : -16.585274342748697  max: 226.87161951563195  min: -427.9770642216005  best/100 : 25  games>0 / 100:  37\n",
            " total time taken (min):  17.363772869110107\n",
            "current  400  average of last 100 episodes : 56.33573502067995  max: 256.58679393764885  min: -356.99964428133023  best/100 : 47  games>0 / 100:  56\n",
            " total time taken (min):  19.07840645313263\n",
            "current  500  average of last 100 episodes : 26.95214097367698  max: 273.24455451373274  min: -433.96189091502447  best/100 : 42  games>0 / 100:  52\n",
            " total time taken (min):  16.700067432721458\n",
            "current  600  average of last 100 episodes : 27.637127104833976  max: 265.80575579190224  min: -473.66960241055733  best/100 : 36  games>0 / 100:  51\n",
            " total time taken (min):  19.08567433754603\n",
            "current  700  average of last 100 episodes : 34.57757518162419  max: 267.4066224961706  min: -434.63707537387654  best/100 : 42  games>0 / 100:  56\n",
            " total time taken (min):  21.910267917315164\n",
            "current  800  average of last 100 episodes : -51.8428726190469  max: 237.11809702340486  min: -521.8330845803113  best/100 : 18  games>0 / 100:  26\n",
            " total time taken (min):  28.9368026693662\n",
            "current  900  average of last 100 episodes : -23.564532325915334  max: 237.31369508851512  min: -499.86221987316674  best/100 : 25  games>0 / 100:  32\n",
            " total time taken (min):  28.01686620314916\n",
            "current  1000  average of last 100 episodes : -26.53534396049333  max: 283.5504658332313  min: -616.0406877727735  best/100 : 27  games>0 / 100:  39\n",
            " total time taken (min):  27.756803520520528\n",
            "current  1100  average of last 100 episodes : 7.608826781794374  max: 261.9945610887693  min: -576.5916381332761  best/100 : 36  games>0 / 100:  50\n",
            " total time taken (min):  27.671281707286834\n",
            "current  1200  average of last 100 episodes : -54.17642875891489  max: 265.32030201135893  min: -384.0845805654664  best/100 : 15  games>0 / 100:  26\n",
            " total time taken (min):  33.72829156716664\n",
            "current  1300  average of last 100 episodes : -73.68316154933491  max: 249.38356556887533  min: -488.8119882665863  best/100 : 11  games>0 / 100:  17\n",
            " total time taken (min):  32.08482704162598\n",
            "current  1400  average of last 100 episodes : -78.44421782944889  max: 268.2626530352609  min: -530.0810365098453  best/100 : 13  games>0 / 100:  25\n",
            " total time taken (min):  22.9916548927625\n",
            "current  1500  average of last 100 episodes : -52.91811661944893  max: 235.36892237673288  min: -460.2351179889653  best/100 : 16  games>0 / 100:  34\n",
            " total time taken (min):  26.717957389354705\n",
            "current  1600  average of last 100 episodes : -68.92088444915916  max: 266.09168011077236  min: -497.88665361216704  best/100 : 14  games>0 / 100:  26\n",
            " total time taken (min):  29.3402081767718\n",
            "current  1700  average of last 100 episodes : -52.91916213355193  max: 267.47981619442794  min: -355.8547775961681  best/100 : 20  games>0 / 100:  28\n",
            " total time taken (min):  24.915251580874124\n",
            "current  1800  average of last 100 episodes : -105.58870507294974  max: 270.0885911860186  min: -474.8462180754514  best/100 : 9  games>0 / 100:  17\n",
            " total time taken (min):  24.63542966047923\n",
            "current  1900  average of last 100 episodes : -27.162796606129067  max: 295.2207242530641  min: -372.45256718127877  best/100 : 24  games>0 / 100:  39\n",
            " total time taken (min):  18.96002090771993\n",
            "current  2000  average of last 100 episodes : 7.118727779813644  max: 272.07534402742215  min: -443.40662194285403  best/100 : 29  games>0 / 100:  42\n",
            " total time taken (min):  15.209239617983501\n",
            "current  2100  average of last 100 episodes : -28.329258583292834  max: 273.6498782249872  min: -457.10227546794096  best/100 : 24  games>0 / 100:  31\n",
            " total time taken (min):  19.74715201854706\n",
            "current  2200  average of last 100 episodes : -11.731804774657313  max: 248.22936511647032  min: -384.20046552164763  best/100 : 26  games>0 / 100:  35\n",
            " total time taken (min):  20.482013575236003\n",
            "current  2300  average of last 100 episodes : -0.9663996184785693  max: 262.40608233405754  min: -373.0019788681881  best/100 : 31  games>0 / 100:  46\n",
            " total time taken (min):  21.091976209481558\n",
            "current  2400  average of last 100 episodes : 18.506243595027566  max: 243.99226679586297  min: -462.7524681906185  best/100 : 38  games>0 / 100:  54\n",
            " total time taken (min):  20.59543512662252\n",
            "current  2500  average of last 100 episodes : 9.91510306904268  max: 282.83728964346335  min: -473.5290817709516  best/100 : 33  games>0 / 100:  44\n",
            " total time taken (min):  20.679429296652476\n",
            "current  2600  average of last 100 episodes : 41.149463486901844  max: 274.11287478216394  min: -441.46319637081217  best/100 : 44  games>0 / 100:  55\n",
            " total time taken (min):  19.213691476980845\n",
            "current  2700  average of last 100 episodes : -11.518680019644707  max: 260.87623730741603  min: -556.6703939429349  best/100 : 39  games>0 / 100:  47\n",
            " total time taken (min):  23.799066857496896\n",
            "current  2800  average of last 100 episodes : 16.01731171094231  max: 257.74452509608966  min: -524.016852013598  best/100 : 49  games>0 / 100:  52\n",
            " total time taken (min):  21.462356499830882\n",
            "current  2900  average of last 100 episodes : 10.954733840356134  max: 272.9844539632979  min: -598.7291853736489  best/100 : 46  games>0 / 100:  52\n",
            " total time taken (min):  22.689809266726176\n",
            "current  3000  average of last 100 episodes : 2.0011697122525693  max: 235.6914592635941  min: -474.68208404743217  best/100 : 33  games>0 / 100:  39\n",
            " total time taken (min):  17.720001379648846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ3aUDTFticV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "apath=\"/ppo_lstm_clean_mean_normal_actormodel64+.h5\"\n",
        "cpath=\"/ppo_lstm_clean_mean_normal_criticmodel64+.h5\"\n",
        "\n",
        "#perfect solve around 700th ep\n",
        "agent_1.actor.save_weights(apath)\n",
        "agent_1.critic.save_weights(cpath)\n",
        "from google.colab import files\n",
        "files.download(apath)\n",
        "files.download(cpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnc230vR3ux9",
        "colab_type": "code",
        "outputId": "6564d747-6914-4486-d003-37e1451b6b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#testing\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "env1=gym.make(env)\n",
        "env1=env1.unwrapped\n",
        "#render = 1\n",
        "for episode in range(1,2):\n",
        "\tdone1=False\n",
        "\tstp=0\n",
        "\ts1=env1.reset()\n",
        "\ts1 = mask_lander(s1)\n",
        "\tagent_1.zero_lstm_state_actor()\n",
        "\t\n",
        "\trews1 = 0\t\n",
        "\twhile not done1:\n",
        "\t\tif stp%(steps//5) ==0:\n",
        "\t\t\tprint(\"+\",end='')\n",
        "\t\tif stp%2 == 0:\n",
        "\t\t\tscreen = env1.render(mode='rgb_array')\n",
        "\t\t\tplt.imshow(screen)\n",
        "\t\t\tplt.pause(0.001)\n",
        "\t\t\tipythondisplay.clear_output(wait=True)\n",
        "\t\t\t#ipythondisplay.display(plt.gcf())c\n",
        "\n",
        "\t\toutput1= agent_1.actor.predict((np.array([front_pad_single(time_horizon,s1)]),DUMMY_VALUE1,DUMMY_ACTION1))\n",
        "\t\t#print(output1)\n",
        "\t\tout1 = output1[0][0][-1]# getting the recent action\n",
        "\t\th = output1[1]\n",
        "\t\tc = output1[2]\n",
        "\t\tmu_pred1,cov_pred1 =out1[:a_dim1],out1[a_dim1:]\n",
        "\t\tcov_pred1 = np.diag(cov_pred1)#*0\n",
        "\t\taction1= np.random.multivariate_normal(mu_pred1,cov_pred1,a_dim1)[0]\n",
        "\t\ta1 =np.clip(action1,-a_bound1,a_bound1 )\n",
        "\t\t#print(a1)\n",
        "\t\ts_1, reward1, done1, info1 = env1.step(a1)\n",
        "\t\ts_1 = mask_lander(s_1)\n",
        "\t\tstp=stp+1\t\n",
        "\t\t#print(\"steps :\",stp)\n",
        "\t\tif stp>steps:\n",
        "\t\t\tdone1=True\t\t\n",
        "\t\tif reward1>99:\n",
        "\t\t\tprint(\"reward \", reward1,\" state loc \",s_1[0],s_1[1],s_1[2],s_1[3],s_1[4])\t\n",
        "\t\t#agent_1.memory.store(s1,action1,val,reward1,done1,out1,cov_pred1)# s, a, s_1, r, done1,musig\n",
        "\t\trews1+=reward1\n",
        "\t\tipythondisplay.clear_output(wait=True)\n",
        "\t\ts1 = s_1\n",
        "\t# updation\n",
        "\tprint(\"|\")\n",
        "\tif episode>00:\n",
        "\t  print(\"final_state \",s_1[0],s_1[1],s_1[2],s_1[3],s_1[4])\n",
        "\t  print(env+\" | \"+str(episode)+\" | \"+str(rews1)+\"|\"+str(stp)+\"| pomdpmask:\"+str(mask)+\" |\"+\"time_horizon :\"+str(time_horizon))\n",
        "\t  print(\"_\"*100)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|\n",
            "final_state  -0.09785843 -0.00081080437 0.0022108702 1.0 0.0\n",
            "LunarLanderContinuous-v2 | 1 | 145.09222705813207|900| pomdpmask:[1. 1. 0. 0. 1. 0. 1. 1.] |time_horizon :64\n",
            "____________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKQx0QYgEOem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "780ec7d6-627a-45f5-c9e1-2580f95e5f20"
      },
      "source": [
        "both left and right thrusters used at same time for better control due to lstm no rocking motion observed unlike mlp on pomdp..\n",
        "when cov = 0 a stair like on off decent/ascent is observed\n",
        "critic 100 times lr \n",
        "c lr = 0.001\n",
        "a lr = 0.0001 at start for few 20k episodes then 0.000001 \n",
        "then a lr = 0.000001   loss clip = 0.05 c lr = 0.001"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-acc5baf1eed5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    both left and right thrusters used at same time for better control due to lstm no rocking motion observed unlike mlp on pomdp..\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A06MyF36_bb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}