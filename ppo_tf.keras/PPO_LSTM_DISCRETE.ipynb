{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_LSTM_DISCRETE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObmGGuTTlWOkxCp75opbEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NonMagneticNeedle/reinforcement_learning/blob/master/ppo_tf.keras/PPO_LSTM_DISCRETE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji6taPR8QxRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-_0PnKJHH3X",
        "colab_type": "code",
        "outputId": "e1c15736-4fbd-40b1-e2ff-b47eb88c818a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "import math\n",
        "import time\n",
        "#import tensorflow_probability as tfp\n",
        "#display gym\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\"\"\"\"\"\"\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "class ppo():\n",
        "\tdef __init__(self,name,s_dim,a_dim,memory,a_bound,time_steps,lstm_units):\n",
        "\t\tself.s_dim = s_dim\n",
        "\t\tself.a_dim =a_dim\n",
        "\t\tself.memory = memory\n",
        "\t\tself.a_bound =a_bound\n",
        "\t\tself.time_steps =time_steps\n",
        "\t\tself.name = name\n",
        "\t\tself.lstm_units= lstm_units\n",
        "\t\tself.actor  = self.make_actor()\n",
        "\t\tself.critic  = self.make_critic()\n",
        "\t\t\n",
        "\tdef make_actor(self):\n",
        "\t\t\t\n",
        "\n",
        "\t\tstate_inputs = tf.keras.Input(batch_shape=(1,self.time_steps,self.s_dim), name='state')\n",
        "\t\tadvantage = tf.keras.Input(batch_shape=(1,self.time_steps,1 ), name=\"Advantage\")\n",
        "\t\told_prediction= tf.keras.Input(batch_shape=(1,self.time_steps,self.a_dim), name=\"old_prediction\")\n",
        "\t\tx = tf.keras.layers.Masking(mask_value=-1.)(state_inputs)\t\n",
        "\t\tx = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(300, activation='relu'))(x)\n",
        "\t\tx = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(300, activation='relu'))(x)\n",
        "\t\tx = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(300, activation='relu'))(x)\n",
        "\t\n",
        "\t\tx,state_h, state_c = tf.keras.layers.LSTM(units= self.lstm_units,return_sequences=True,return_state=True,stateful=True,name='a_lstm',unroll = True)(x)\n",
        "\t\tx = tf.keras.layers.Dense(100, activation='relu')(x)\n",
        "\t\taction_outputs = tf.keras.layers.Dense(self.a_dim, activation='softmax',name = 'policy_head')(x)\n",
        "\t\t\n",
        "\t\tdef proximal_policy_optimization_loss(advantage, old_prediction):\n",
        "\t\t\tloss_clipping = 0.2\n",
        "\t\t\tentropy_loss = 0.01\n",
        "\t\t\tadvantage = advantage[0]\n",
        "\t\t\told_prediction = old_prediction[0]\n",
        "\t\t\t#y_true = one hot actions , y_pred = prob output\n",
        "\t\t\tdef loss(y_true, y_pred):\n",
        "\t\t\t\t#y_true is the one hot action taken\n",
        "\t\t\t\ty_true = y_true[0]\n",
        "\t\t\t\ty_pred = y_pred[0]\n",
        "\t\t\t\tprob = y_true * y_pred\n",
        "\t\t\t\told_prob = y_true * old_prediction\n",
        "\t\t\t\tr = prob / (old_prob + 1e-10)\n",
        "\t\t\t\tloss = -tf.keras.backend.mean(tf.keras.backend.minimum(r * advantage, tf.keras.backend.clip(r, min_value=1 - loss_clipping,max_value=1 + loss_clipping) * advantage)) #+ entropy_loss * (prob * tf.keras.backend.log(prob + 1e-10)))\n",
        "\t\t\t\treturn loss\n",
        "\t\t\treturn loss\t\n",
        "\t\tactor_model= tf.keras.Model(inputs=[state_inputs, advantage,old_prediction], outputs=[action_outputs,state_h,state_c], name='actor_model')\n",
        "\t\tactor_model.compile(loss={'policy_head':proximal_policy_optimization_loss(advantage=advantage,old_prediction=old_prediction)} ,optimizer=tf.keras.optimizers.Adam(lr=0.0001))\n",
        "\t\treturn actor_model\n",
        "\tdef make_critic(self):\n",
        "\t\t\t\n",
        "\n",
        "\t\tstate_inputs = tf.keras.Input(batch_shape=(1,self.time_steps,self.s_dim), name='state')\n",
        "\t\tx = tf.keras.layers.Masking(mask_value=-1.)(state_inputs)\t\n",
        "\t\tx = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(300, activation='relu'))(x)\n",
        "\t\tx = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(300, activation='relu'))(x)\n",
        "\t\tx = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(300, activation='relu'))(x)\n",
        "\t\tx,state_h, state_c = tf.keras.layers.LSTM(units= self.lstm_units,return_sequences=True,return_state=True,stateful=True,name='c_lstm',unroll =True)(x)\n",
        "\t\tx = tf.keras.layers.Dense(100, activation='relu')(x)\t\n",
        "\t\tval = tf.keras.layers.Dense(1, activation=None,name = \"value_head\",use_bias=True)(x)\n",
        "\t\t#hope is msaking makes skips the gradients too # still it trains the layers (after lstms) bias value even if the loss is 0 due to mask\n",
        "\t\tcritic_model= tf.keras.Model(inputs=state_inputs, outputs=[val,state_h,state_c], name='critic_model')\n",
        "\t\tcritic_model.compile(loss={'value_head':'mse'} ,optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
        "\t\treturn critic_model\n",
        "\tdef set_lstm_state_actor(self,h,c):\n",
        "\t\tself.actor.get_layer('a_lstm').reset_states([h,c])\n",
        "\tdef zero_lstm_state_actor(self):\n",
        "\t\tc_0 = np.zeros((1,self.lstm_units)).astype(np.float32)\n",
        "\t\th_0 = np.zeros((1,self.lstm_units)).astype(np.float32)\n",
        "\t\tself.actor.get_layer('a_lstm').reset_states([h_0,c_0])\n",
        "\n",
        "\tdef set_lstm_state_critic(self,h,c):\n",
        "\t\tself.critic.get_layer('a_lstm').reset_states([h,c])\n",
        "\tdef zero_lstm_state_critic(self):\n",
        "\t\tc_0 = np.zeros((1,self.lstm_units)).astype(np.float32)\n",
        "\t\th_0 = np.zeros((1,self.lstm_units)).astype(np.float32)\n",
        "\t\tself.critic.get_layer('c_lstm').reset_states([h_0,c_0])\n",
        "\tdef onehot(self,a):\n",
        "\t\ti = np.zeros(self.a_dim)\n",
        "\t\ti[a]=1\n",
        "\t\treturn i\t\t\n",
        "\t\n",
        "\tdef gae_calc(self,val,val_,rew,done):\n",
        "\t\tmask=1 \n",
        "\t\tgae=0\n",
        "\t\tgamma=0.99\n",
        "\t\tlambd = 0.95\n",
        "\t\treturns=np.zeros_like(val)\n",
        "\t\tfor i in reversed(range(0,len(val))):\n",
        "\t\t\tmask=1\n",
        "\t\t\tif done[i]:\n",
        "\t\t\t\tmask = 0 \t\n",
        "\t\t\tdelta=rew[i]+gamma*val_[i]*mask - val[i]\n",
        "\t\t\tgae=delta+gamma*lambd*mask*gae\n",
        "\t\t\treturns[i]=gae+val[i]\n",
        "\t\treturn returns\n",
        "\t\t\n",
        "\tdef adv_calc(self,val,val_,rew,done):\n",
        "\t\tgamma=0.99\n",
        "\t\treturns=np.zeros_like(val)\n",
        "\t\tfor i in range(0,len(val)):\n",
        "\t\t\treturns[i] = rew[i] + (1- done[i])*val_[i]*gamma\n",
        "\t\treturn returns\n",
        "\tdef batchify(self,lst):\n",
        "\t\tdef ceil(n):\n",
        "\t\t\tres = int(n)\n",
        "\t\t\treturn res if res == n or n < 0 else res+1\t\n",
        "\t\tdef front_pad_batch(batchsize,data):\n",
        "\t\t\td = np.zeros((batchsize,data.shape[-1]))-1.\n",
        "\t\t\td[batchsize-data.shape[0]:] = data\n",
        "\t\t\treturn d\t\n",
        "\t\tbatchsize =self.time_steps\n",
        "\t\tfull_size = ceil(lst.shape[0]/batchsize)*batchsize\n",
        "\n",
        "\t\treturn front_pad_batch(full_size,lst)\t\n",
        "\tdef batchify_adv(self,lst):\n",
        "\t\tdef ceil(n):\n",
        "\t\t\tres = int(n)\n",
        "\t\t\treturn res if res == n or n < 0 else res+1\t\n",
        "\t\tdef front_pad_batch(batchsize,data):\n",
        "\t\t\td = np.zeros((batchsize,data.shape[-1]))\n",
        "\t\t\td[batchsize-data.shape[0]:] = data\n",
        "\t\t\treturn d\t\n",
        "\t\tbatchsize =self.time_steps\n",
        "\t\tfull_size = ceil(lst.shape[0]/batchsize)*batchsize\n",
        "\n",
        "\t\treturn front_pad_batch(full_size,lst)\t\t\t\n",
        "\tdef batchify_oldpred(self,lst):\n",
        "\t\tdef ceil(n):\n",
        "\t\t\tres = int(n)\n",
        "\t\t\treturn res if res == n or n < 0 else res+1\t\n",
        "\t\tdef front_pad_batch(batchsize,data):\n",
        "\t\t\td = np.zeros((batchsize,data.shape[-1]))+1\n",
        "\t\t\td[batchsize-data.shape[0]:] = data\n",
        "\t\t\treturn d\t\n",
        "\t\tbatchsize =self.time_steps\n",
        "\t\tfull_size = ceil(lst.shape[0]/batchsize)*batchsize\n",
        "\n",
        "\t\treturn front_pad_batch(full_size,lst)\t\n",
        "\tdef train(self,epochs=10):\n",
        "\t\ttime_horizon=self.time_steps\n",
        "\n",
        "\t\tobs =np.array(self.memory.batch_s)\n",
        "\t\tobs_ =np.array(self.memory.batch_s_)\n",
        "\t\ths =np.array(self.memory.batch_h)\t\n",
        "\t\tcs =np.array(self.memory.batch_c)\n",
        "\t\tall_obs=self.memory.batch_s.copy()\n",
        "\t\tall_obs.append(self.memory.batch_s_[-1]  )   \n",
        "\t\tall_obs = np.array(all_obs)\n",
        "\t\tall_obs_no = all_obs.shape[0]    \n",
        "\t\tall_obs_batch = self.batchify(all_obs)               \n",
        "\t\tall_values = np.zeros((all_obs_batch.shape[0],1))\n",
        "\t\tself.zero_lstm_state_critic()    \n",
        "\t\tfor b in range(0,all_obs_batch.shape[0],time_horizon):\n",
        "\t\t\tvb ,h,c=\tself.critic.predict(x=(np.array([all_obs_batch[b:b+time_horizon]])))\n",
        "\t\t\tall_values[b:b+time_horizon] = vb[0]\n",
        "\t\t\t\n",
        "\t\tvalues =all_values[-all_obs_no:-1] \n",
        "\t\tvalues_ = all_values[-all_obs_no+1:]\t\n",
        "\t\t#returns = self.adv_calc(values,values_,self.memory.batch_r,self.memory.batch_done) ############################################################\n",
        "\t\treturns = self.gae_calc(values,values_,self.memory.batch_r,self.memory.batch_done) #############################################################\n",
        "\t\tadvantage=np.array(returns-values)\n",
        "\t\tAction=np.array(self.memory.batch_a)\n",
        "\t\tOld_Prediction =np.array(self.memory.pred)\n",
        "\t\t#print(obs.shape,advantage.shape,Action.shape,Old_Prediction_musig.shape,returns.shape)\t\t\n",
        "\t\tobs = self.batchify(obs)\n",
        "\t\tadvantage = self.batchify_adv(advantage)\n",
        "\t\tAction =self.batchify_adv(Action)\n",
        "\t\tOld_Prediction =self.batchify_oldpred(Old_Prediction)\n",
        "\t\treturns =self.batchify(returns)\n",
        "\t\t#print(obs.shape,advantage.shape,Action.shape,Old_Prediction.shape,returns.shape)\n",
        "\t\t#print(returns,obs)\n",
        "\t\tfor ep in range(epochs):\n",
        "\t\t\tself.zero_lstm_state_actor()\n",
        "\t\t\tself.zero_lstm_state_critic()\n",
        "\t\t\tfor b in range(0,obs.shape[0],time_horizon):\n",
        "\t\t\t\t#train actor\n",
        "\t\t\t\tself.actor.fit(x=(np.array([obs[b:b+time_horizon]]),np.array([advantage[b:b+time_horizon]]),np.array([Old_Prediction[b:b+time_horizon]])),y={'policy_head':np.array([Action[b:b+time_horizon]])},shuffle=False, epochs=1, verbose=0)\n",
        "\n",
        "\t\t\t\t#train critic\t\n",
        "\t\t\t\tself.critic.fit(x=(np.array([obs[b:b+time_horizon]])),y={'value_head':np.array([returns[b:b+time_horizon]])},shuffle=False, epochs=1, verbose=0)\n",
        "\t\tself.memory.clear()\n",
        "\n",
        "\n",
        "\n",
        "class Memory:\n",
        "\tdef __init__(self):\n",
        "\t\tself.batch_s = []\n",
        "\t\tself.batch_a = []\n",
        "\t\tself.batch_r = []\n",
        "\t\tself.batch_s_ = []\n",
        "\t\tself.batch_h = []\n",
        "\t\tself.batch_c =[]\n",
        "\t\tself.batch_done = []\n",
        "\t\tself.pred = []\n",
        "\tdef store(self, s, a, s_, r, done,pred,h,c):\n",
        "\t\tself.batch_s.append(s)\n",
        "\t\tself.batch_a.append(a)\n",
        "\t\tself.batch_r.append(r)\n",
        "\t\tself.batch_s_.append(s_)\n",
        "\t\tself.batch_done.append(done)\n",
        "\t\tself.pred.append(pred)\n",
        "\t\tself.batch_h.append(h)\n",
        "\t\tself.batch_c.append(c)\n",
        "\tdef clear(self):\n",
        "\t\tself.batch_s.clear()\n",
        "\t\tself.batch_a.clear()\n",
        "\t\tself.batch_r.clear()\n",
        "\t\tself.batch_s_.clear()\n",
        "\t\tself.batch_done.clear()\n",
        "\t\tself.pred.clear()\n",
        "\t\tself.batch_h.clear()\n",
        "\t\tself.batch_c.clear()\n",
        "\tdef cnt_samples(self):\n",
        "\t\treturn len(self.batch_s)\n",
        "\n",
        "\n",
        "def front_pad_single(batchsize,data):\n",
        "\td = np.zeros((batchsize,data.shape[-1]))-1.\n",
        "\t#print(\"d \",d.shape ,\"data\",data.shape)\n",
        "\td[-1] = data\n",
        "\t#print(\"padded sequence\",d)\n",
        "\treturn d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "envs = ['CartPole-v1']\n",
        "\n",
        "env= envs[0]\n",
        "env1=gym.make(env)\n",
        "env1=env1.unwrapped\n",
        "\n",
        "s_dim1 =env1.observation_space.shape[0]\n",
        "print(s_dim1)\n",
        "a_dim1 =env1.action_space.n\n",
        "print(a_dim1)\n",
        "a_bound1 = 1#env1.action_space.high[0]\n",
        "print(a_bound1)\n",
        "\n",
        "memory_1=Memory()\n",
        "time_horizon=64##########################\n",
        "\n",
        "agent_1 =  ppo(name = \"ppo_agent_01\",s_dim=s_dim1 ,a_dim= a_dim1,memory = memory_1,a_bound=a_bound1,time_steps=time_horizon,lstm_units=100)\n",
        "\n",
        "def preprocess(state):\n",
        "\t#return np.array([state[0],state[1],state[4],state[6],state[7]])\n",
        "\treturn state\n",
        "\n",
        "\n",
        "\n",
        "DUMMY_ACTION1, DUMMY_VALUE1 ,DUMMY_LOSS_MASK = np.zeros((1,time_horizon,a_dim1)), np.zeros((1,time_horizon, 1)),np.zeros((1,time_horizon, 1))\n",
        "\n",
        "#check if final hidden states are consistnent oor not by taking time as 1,2,5,10 \n",
        "#find vs_ of terminal state\n",
        "episodes = 30000\n",
        "steps = 200\n",
        "render=0\n",
        "print(\"starting>>>\")\n",
        "\n",
        "\n",
        "\n",
        "apath=\"/gdrive/My Drive/ll_clean/ppo_lstm_clean_mean_tiny_actormodel64.h5\"\n",
        "cpath=\"/gdrive/My Drive/ll_clean/ppo_lstm_clean_mean_tiny_criticmodel64.h5\"\n",
        "#agent_1.actor.load_weights(apath)\n",
        "#agent_1.critic.load_weights(cpath)\n",
        "print(\"loaded\")\n",
        "avg =[]\n",
        "t = time.time()\n",
        "best = 0 \n",
        "z =0\n",
        "for episode in range(1,episodes):\n",
        "\tdone1=False\n",
        "\tstp=0\n",
        "\ts1=env1.reset()\n",
        "\ts1 = preprocess(s1)\n",
        "\tagent_1.zero_lstm_state_actor()\n",
        "\t\n",
        "\trews1 = 0\t\n",
        "\tif episode > 500:\n",
        "\t\trender=0\n",
        "\n",
        "\twhile not done1:\n",
        "\t\t#if stp%(steps//10) ==0:\n",
        "\t\t#\tprint(\"+\",end='')\n",
        "\t\tif render:\n",
        "\t\t\tenv1.render()\t\n",
        "\t\toutput1= agent_1.actor.predict((np.array([front_pad_single(time_horizon,s1)]),DUMMY_VALUE1,DUMMY_ACTION1))\n",
        "\t\t#print(output1)\n",
        "\t\tpred_action = output1[0][0][-1]# getting the recent action\n",
        "\t\th = output1[1]\n",
        "\t\tc = output1[2]\n",
        "\t\t#print(pred_action,pred_action.shape)\n",
        "\t\taction = np.random.choice(np.arange(pred_action.shape[0]), p=pred_action.ravel())# action chosen\n",
        "\t\taction_one_hot= agent_1.onehot(action)# acton matrix\n",
        "\t\ts_1, reward1, done1, info1 = env1.step(action)\n",
        "\t\t#print(\"s_1\",s_1, end='')\t\n",
        "\t\ts_1 = preprocess(s_1)\n",
        "\t\t#print(\" sub:\",s_1-s1)\n",
        "\t\tstp=stp+1\t\n",
        "\n",
        "\n",
        "\t\tif (stp>steps):# or (s_1[3]+s_1[4]>1):\n",
        "\t\t\tdone1=True\t\t\n",
        "\t\tagent_1.memory.store(s1,action_one_hot ,s_1,reward1,done1,pred_action.ravel(),h,c)# state, selected onehot action, next state, reward , done ,predicted action, h , c\n",
        "\t\trews1+=reward1\n",
        "\n",
        "\t\ts1 = s_1\n",
        "\tavg.append(rews1)\n",
        "\tif rews1>100:\n",
        "\t\tbest+=1 \n",
        "\tif rews1>0:\n",
        "\t\tz+=1 \t\t\n",
        "\tagent_1.train()\n",
        "\tif episode % 100 == 0:\n",
        "\t\tprint(\"current \",episode , \" average of last 100 episodes :\", np.mean(np.array(avg)), \" max:\", np.max(np.array(avg)), \" min:\", np.min(np.array(avg)), \" more than 100 games:\", best,\" more than 0 games:\", z )\n",
        "\t\tz=0\n",
        "\t\tbest = 0\n",
        "\t\tavg.clear()\n",
        "\t\n",
        "\n",
        "\t\tapath=\"/gdrive/My Drive/ll_clean/ppo_lstm_clean_mean_tiny_actormodel64.h5\"\n",
        "\t\tcpath=\"/gdrive/My Drive/ll_clean/ppo_lstm_clean_mean_tiny_criticmodel64.h5\"\n",
        "\n",
        "\t\t#perfect solve around 700th ep\n",
        "\t\t#agent_1.actor.save_weights(apath)\n",
        "\t\t#agent_1.critic.save_weights(cpath)\n",
        "\t\tprint(\" total time taken (min): \" ,(time.time()-t)/60)\n",
        "\t\tt = time.time()\n",
        "\n",
        "\t\t "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "4\n",
            "2\n",
            "1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Output a_lstm missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to a_lstm.\n",
            "WARNING:tensorflow:Output a_lstm_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to a_lstm_1.\n",
            "WARNING:tensorflow:Output c_lstm missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to c_lstm.\n",
            "WARNING:tensorflow:Output c_lstm_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to c_lstm_1.\n",
            "starting>>>\n",
            "loaded\n",
            "current  100  average of last 100 episodes : 148.87  max: 201.0  min: 9.0  more than 100 games: 75  more than 0 games: 100\n",
            " total time taken (min):  5.1113889137903845\n",
            "current  200  average of last 100 episodes : 156.58  max: 201.0  min: 72.0  more than 100 games: 94  more than 0 games: 100\n",
            " total time taken (min):  4.975412770112356\n",
            "current  300  average of last 100 episodes : 140.12  max: 201.0  min: 58.0  more than 100 games: 84  more than 0 games: 100\n",
            " total time taken (min):  4.395323395729065\n",
            "current  400  average of last 100 episodes : 133.47  max: 201.0  min: 63.0  more than 100 games: 82  more than 0 games: 100\n",
            " total time taken (min):  4.2390475114186605\n",
            "current  500  average of last 100 episodes : 152.72  max: 201.0  min: 26.0  more than 100 games: 93  more than 0 games: 100\n",
            " total time taken (min):  4.817508323987325\n",
            "current  600  average of last 100 episodes : 176.98  max: 201.0  min: 118.0  more than 100 games: 100  more than 0 games: 100\n",
            " total time taken (min):  5.508315471808116\n",
            "current  700  average of last 100 episodes : 171.12  max: 201.0  min: 93.0  more than 100 games: 99  more than 0 games: 100\n",
            " total time taken (min):  5.2260114828745525\n",
            "current  800  average of last 100 episodes : 144.85  max: 201.0  min: 81.0  more than 100 games: 88  more than 0 games: 100\n",
            " total time taken (min):  4.44288881222407\n",
            "current  900  average of last 100 episodes : 149.82  max: 201.0  min: 81.0  more than 100 games: 88  more than 0 games: 100\n",
            " total time taken (min):  4.657037766774495\n",
            "current  1000  average of last 100 episodes : 164.32  max: 201.0  min: 92.0  more than 100 games: 98  more than 0 games: 100\n",
            " total time taken (min):  4.887147823969523\n",
            "current  1100  average of last 100 episodes : 168.43  max: 201.0  min: 94.0  more than 100 games: 99  more than 0 games: 100\n",
            " total time taken (min):  5.15627890030543\n",
            "current  1200  average of last 100 episodes : 166.31  max: 201.0  min: 101.0  more than 100 games: 100  more than 0 games: 100\n",
            " total time taken (min):  5.295448501904805\n",
            "current  1300  average of last 100 episodes : 166.28  max: 201.0  min: 124.0  more than 100 games: 100  more than 0 games: 100\n",
            " total time taken (min):  5.284653309981028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a93543873198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0menv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                 \u001b[0moutput1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0magent_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfront_pad_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_horizon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDUMMY_VALUE1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDUMMY_ACTION1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m                 \u001b[0;31m#print(output1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mpred_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m# getting the recent action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mmake_logs\u001b[0;34m(model, logs, outputs, mode, prefix)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0;34m\"\"\"Computes logs for sending to `on_batch_end` methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m   \u001b[0mmetric_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmetric_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mmetrics_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;31m# Add all metric names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     \u001b[0mmetrics_names\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m       \u001b[0mmetrics\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_metric_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m     \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_metrics_from_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_get_metrics_from_layers\u001b[0;34m(layers)\u001b[0m\n\u001b[1;32m   3172\u001b[0m       \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_metrics_from_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3174\u001b[0;31m       \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3175\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0mcollected_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0mall_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_unique_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0mcollected_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m_gather_unique_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2338\u001b[0m     \u001b[0mWe\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdeduping\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mgetting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmaintain\u001b[0m \u001b[0mthe\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \"\"\"\n\u001b[0;32m-> 2340\u001b[0;31m     \u001b[0mall_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2341\u001b[0m     \u001b[0munique_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2342\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m_gather_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2353\u001b[0m       child_layers = trackable_layer_utils.filter_empty_layer_containers(\n\u001b[1;32m   2354\u001b[0m           self._layers)\n\u001b[0;32m-> 2355\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mchild_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m         \u001b[0mall_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mfilter_empty_layer_containers\u001b[0;34m(layer_list)\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0;34m\"\"\"Filter out empty Layer-like containers and uniquify.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;31m# TODO(b/130381733): Make this an attribute in base_layer.Layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m   \u001b[0mexisting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m   \u001b[0mto_visit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mto_visit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/object_identity.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}